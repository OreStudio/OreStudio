:PROPERTIES:
:ID: B13DD349-0341-416E-8A31-6765A9EFCD6F
:END:
#+title: Sprint Backlog 09
#+options: <:nil c:nil ^:nil d:nil date:nil author:nil toc:nil html-postamble:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED BLOCKED
#+tags: { code(c) infra(i) analysis(n) agile(a) }
#+startup: inlineimages

* Sprint Mission

- continue reference data.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :tags t :indent nil :emphasize nil :scope file :narrow 75 :formula % :block today
#+TBLNAME: today_summary
#+CAPTION: Clock summary at [2026-01-18 Sun 20:44], for Sunday, January 18, 2026.
|       | <75>                               |        |      |      |       |
| Tags  | Headline                           | Time   |      |      |     % |
|-------+------------------------------------+--------+------+------+-------|
|       | *Total time*                       | *6:16* |      |      | 100.0 |
|-------+------------------------------------+--------+------+------+-------|
|       | Stories                            | 6:16   |      |      | 100.0 |
|       | Active                             |        | 6:16 |      | 100.0 |
| infra | OCR scan notebooks for this sprint |        |      | 1:33 |  24.7 |
| code  | Add UI support for data sets       |        |      | 4:43 |  75.3 |
#+end:

#+begin: clocktable :maxlevel 3 :scope subtree :tags t :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+TBLNAME: sprint_summary
#+CAPTION: Clock summary at [2026-01-18 Sun 20:44]
|       | <75>                                               |         |       |       |       |
| Tags  | Headline                                           | Time    |       |       |     % |
|-------+----------------------------------------------------+---------+-------+-------+-------|
|       | *Total time*                                       | *59:00* |       |       | 100.0 |
|-------+----------------------------------------------------+---------+-------+-------+-------|
|       | Stories                                            | 59:00   |       |       | 100.0 |
|       | Active                                             |         | 59:00 |       | 100.0 |
| agile | Sprint and product backlog refinement              |         |       |  2:11 |   3.7 |
| infra | OCR scan notebooks for this sprint                 |         |       |  8:19 |  14.1 |
| infra | Clean up system models and component models        |         |       |  0:28 |   0.8 |
| code  | Add missing entities to CLI                        |         |       |  1:55 |   3.2 |
| infra | Refactor PR related skills                         |         |       |  0:18 |   0.5 |
| code  | Fix valgrind errors in nightly                     |         |       |  1:25 |   2.4 |
| code  | Create security module                             |         |       |  1:23 |   2.3 |
| code  | Prodigy should support multiple environments       |         |       |  0:19 |   0.5 |
| code  | Add named environment sets                         |         |       |  4:17 |   7.3 |
| code  | Rename risk to refdata                             |         |       |  0:43 |   1.2 |
| code  | Prefix database entities with component            |         |       |  3:15 |   5.5 |
| code  | Implement data quality                             |         |       |  9:42 |  16.4 |
| code  | Improvements to connection browser                 |         |       |  2:05 |   3.5 |
| code  | Disconnect shows reconnect message                 |         |       |  2:24 |   4.1 |
| infra | Refactor documentation                             |         |       |  6:29 |  11.0 |
| code  | Add UI support for data sets                       |         |       | 13:27 |  22.8 |
| code  | Implement party related entities at database level |         |       |  0:20 |   0.6 |
#+end:

*** STARTED Sprint and product backlog refinement                     :agile:
    :LOGBOOK:
    CLOCK: [2026-01-16 Fri 11:04]--[2026-01-16 Fri 11:36] =>  0:32
    CLOCK: [2026-01-11 Sun 22:00]--[2026-01-11 Sun 23:39] =>  1:39
    :END:

Updates to sprint and product backlog.

#+begin_src emacs-lisp :exports none
;; agenda
(org-agenda-file-to-front)
#+end_src

#+name: pie-stories-chart
#+begin_src R :var sprint_summary=sprint_summary :colnames yes :results file graphics :exports results :file sprint_backlog_09_stories_pie_sorted.png :width 1920 :height 1080
library(conflicted)
library(ggplot2)
library(tidyverse)
library(tibble)

# Filter to only rows with actual story data (non-empty Tags column)
clean_sprint_summary <- sprint_summary %>% dplyr::filter(!is.na(Tags) & nzchar(Tags))
stories <- unlist(clean_sprint_summary[2])
percent_values <- as.numeric(unlist(clean_sprint_summary[6]))

# Create a data frame and explicitly sort the stories by defining factor levels
df <- data.frame(
  stories = stories,
  percent = percent_values
) %>%
  # 1. Sort the data frame by percentage in descending order
  arrange(desc(percent)) %>%
  # 2. Convert 'stories' to a factor, setting the levels based on the sorted order.
  # This makes the order of the slices explicit for ggplot.
  mutate(
    stories = factor(stories, levels = stories),
    lab.pos = cumsum(percent) - 0.5 * percent
  )

# Manually selected colors to resemble the screenshot
custom_palette <- c(
  "#21518f", "#f37735", "#ffc425", "#81b214", "#d7385e",
  "#662e91", "#00a9ae", "#5c5c5c", "#a0c6e0", "#f8b195",
  "#ffe385", "#bde0fe", "#c5e0d4", "#e0b8a0", "#a56f8f",
  "#7a448a", "#4a9a9b", "#9b9b9b", "#6fa8dc", "#f7a072",
  "#ffd166", "#99d98c", "#ef5d60", "#9d529f", "#3a86ff",
  "#c1d6e1", "#f9e0ac", "#c2d6a4", "#e69a8d", "#a07d9f"
)

# Ensure the palette has enough colors for all stories.
if (length(custom_palette) < length(df$stories)) {
  warning("Not enough custom colors for all stories. Colors will repeat.")
  custom_palette <- rep(custom_palette, length.out = length(df$stories))
}


p <- ggplot(df, aes(x = "", y = percent, fill = stories)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = custom_palette) +
  ggtitle("Sprint 9: Resourcing per Story")  +
  labs(x = NULL, y = NULL, fill = "Stories") +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 18),
    legend.position = "right",
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )

print(p)
#+end_src

#+RESULTS: pie-stories-chart
[[file:sprint_backlog_09_stories_pie_sorted.png]]

#+name: stories-chart
#+begin_src R :var sprint_summary=sprint_summary :colnames yes :results file graphics :exports results :file sprint_backlog_09_stories.png :width 1200 :height 650
library(conflicted)
library(grid)
library(tidyverse)
library(tibble)

# Filter to only rows with actual story data (non-empty Tags column)
clean_sprint_summary <- sprint_summary %>% dplyr::filter(Tags != "")
names <- unlist(clean_sprint_summary[2])
values <- as.numeric(unlist(clean_sprint_summary[6]))

# Create a data frame.
df <- data.frame(
  cost = values,
  stories = factor(names, levels = names[order(values, decreasing = FALSE)]),
  y = seq(length(names)) * 0.9
)

# Setup the colors
blue <- "#076fa2"

p <- ggplot(df) +
  aes(x = cost, y = stories) +
  geom_col(fill = blue, width = 0.6) +
  ggtitle("Sprint 9: Resourcing per Story") +
  xlab("Resourcing (%)") + ylab("Stories") +
  theme(text = element_text(size = 15))

print(p)
#+end_src

#+RESULTS: stories-chart
[[file:sprint_backlog_09_stories.png]]

#+name: tags-chart
#+begin_src R :var sprint_summary=sprint_summary :colnames yes :results file graphics :exports results :file sprint_backlog_09_tags.png :width 600 :height 400
library(conflicted)
library(grid)
library(tidyverse)
library(tibble)

# Filter to only rows with actual story data (non-empty Tags column)
clean_sprint_summary <- sprint_summary %>% dplyr::filter(Tags != "")
names <- unlist(clean_sprint_summary[1])
values <- as.numeric(unlist(clean_sprint_summary[6]))

# Create a data frame.
df <- data.frame(
  cost = values,
  tags = names,
  y = seq(length(names)) * 0.9
)
# factor(names, levels = names[order(values, decreasing = FALSE)])

df2 <- setNames(aggregate(df$cost, by = list(df$tags), FUN = sum),  c("cost", "tags"))
# Setup the colors

blue <- "#076fa2"

p <- ggplot(df2) +
  aes(x = cost, y = tags) +
  geom_col(fill = blue, width = 0.6) +
  ggtitle("Sprint 9: Resourcing per Tag") +
  xlab("Resourcing (%)") + ylab("Story types") +
  theme(text = element_text(size = 15))

print(p)
#+end_src

#+RESULTS: tags-chart
[[file:sprint_backlog_09_tags.png]]

*** STARTED OCR scan notebooks for this sprint                        :infra:
    :LOGBOOK:
    CLOCK: [2026-01-18 Sun 11:22]--[2026-01-18 Sun 12:55] =>  1:33
    CLOCK: [2026-01-16 Fri 09:30]--[2026-01-16 Fri 11:03] =>  1:33
    CLOCK: [2026-01-15 Thu 23:31]--[2026-01-15 Thu 23:55] =>  0:24
    CLOCK: [2026-01-15 Thu 09:47]--[2026-01-15 Thu 11:30] =>  1:43
    CLOCK: [2026-01-15 Thu 09:28]--[2026-01-15 Thu 09:46] =>  0:18
    CLOCK: [2026-01-14 Wed 12:06]--[2026-01-14 Wed 12:30] =>  0:24
    CLOCK: [2026-01-14 Wed 09:16]--[2026-01-14 Wed 09:40] =>  0:24
    CLOCK: [2026-01-13 Tue 11:00]--[2026-01-13 Tue 11:59] =>  0:59
    CLOCK: [2026-01-13 Tue 09:40]--[2026-01-13 Tue 10:41] =>  1:01
    :END:

We need to scan all of our finance notebooks so we can use them with AI. Each
sprint will have a story similar to this until we scan and process them all.


*** COMPLETED Clean up system models and component models             :infra:
    :LOGBOOK:
    CLOCK: [2026-01-11 Sun 23:40]--[2026-01-12 Mon 00:08] =>  0:28
    :END:

These are out of sync with the latest refactorings.

#+begin_quote
This pull request significantly enhances the project's architectural
documentation and internal tooling. It introduces a new skill to streamline the
creation of component models, integrates several previously undocumented
components into the system's architectural overview, and refines existing
documentation practices to improve clarity and maintainability. The changes aim
to provide a more comprehensive and accessible understanding of the system's
structure and its various modules.

Highlights

- New Component Model Creator Skill: A new skill, 'component-model-creator', has
  been introduced to standardize the process of creating org-mode component
  documentation and PlantUML diagrams for architectural clarity.
- Expanded Component Documentation: Detailed component models and PlantUML
  diagrams have been created for 'ores.logging', 'ores.http',
  'ores.http.server', and 'ores.wt', providing comprehensive insights into their
  architecture and functionality.
- Updated System Architecture: The main system model and architecture diagram
  ('ores.puml') have been updated to include the newly documented components and
  their interdependencies, offering a more complete system overview.
- Enhanced Skill Management Guidelines: The 'skill-manager' documentation now
  includes explicit guidelines for updating the skill dependencies diagram,
  ensuring consistency and ease of maintenance for skill relationships.
- Simplified Doxygen Main Page: The Doxygen main page has been streamlined,
  removing verbose content and instead linking directly to the external website
  documentation for system models and build instructions, improving navigation
  and reducing redundancy.
#+end_quote

*** COMPLETED Add missing entities to CLI                              :code:
    :LOGBOOK:
    CLOCK: [2026-01-12 Mon 10:15]--[2026-01-12 Mon 11:11] =>  0:56
    CLOCK: [2026-01-12 Mon 09:15]--[2026-01-12 Mon 10:14] =>  0:59
    :END:

We have added entities but not updated the tool

**** CLI commands

#+begin_quote
This pull request significantly expands the CLI's administrative capabilities by
introducing comprehensive management commands for several key entities. Users
can now list and delete roles, permissions, countries, change reasons, and
change reason categories directly from the command line, enhancing operational
efficiency and data control. The changes are implemented with flexible output
formats and robust lookup mechanisms, providing a more complete and
user-friendly experience for managing these system components.

Highlights:

- New CLI Entities: Added command-line interface support for five new entities:
  roles, permissions, countries, change reasons, and change reason categories.
- List Operations: Implemented 'list' operations for all new entities,
  supporting both JSON and table output formats.
- Delete Operations: Implemented 'delete' operations for all new entities, with
  support for lookup by UUID, code, or name depending on the entity type.
- Code Structure: Introduced new parser headers and implementations for each new
  entity, and integrated them into the existing application and parser logic.
#+end_quote

**** Parser refactoring

#+begin_quote
This pull request significantly improves the maintainability and conciseness of
the CLI's entity parsing logic. By abstracting common patterns for entities that
only support 'list' and 'delete' operations into a generic helper, it eliminates
substantial boilerplate code and provides a standardized approach for handling
such commands. This change is a direct response to previous code review feedback
aimed at reducing duplication.

Highlights:

- New Generic Helper: Introduced a new simple_entity_config struct and
  handle_simple_entity_command helper function in parser_helpers to centralize
  common parsing logic.
- Refactoring of Entity Parsers: Refactored five existing entity parsers (roles,
  permissions, countries, change_reasons, change_reason_categories) to utilize
  the newly created generic helper.
- Code Reduction: Achieved a net reduction of approximately 314 lines of
  duplicated code across the refactored parsers, improving maintainability and
  conciseness.

#+end_quote

*** COMPLETED Refactor PR related skills                              :infra:
    :LOGBOOK:
    CLOCK: [2026-01-12 Mon 11:12]--[2026-01-12 Mon 11:30] =>  0:18
    :END:

#+begin_quote
This pull request streamlines and centralizes the Pull Request (PR) management
process by renaming and significantly expanding the 'pr-summary' skill into a
comprehensive 'pr-manager'. This change consolidates the entire PR lifecycle,
from initial draft creation and CI integration to final merging, into a single,
reusable skill. By doing so, it eliminates redundant PR creation logic
previously found in other developer skills, promoting a more efficient and
standardized workflow across the system.

Highlights:

- Skill Renaming and Consolidation: The 'pr-summary' skill has been renamed to
  'pr-manager' and its scope significantly expanded to manage the entire Pull
  Request (PR) lifecycle.
- Centralized PR Workflow: The new 'pr-manager' skill now encompasses a
  comprehensive 5-step PR workflow: generating a summary, creating a draft PR,
  waiting for CI to pass, marking the PR as ready for review, and finally
  merging the PR.
- Standardized PR Body Format: A detailed and structured PR body format template
  has been integrated into the 'pr-manager' skill, promoting consistency across
  PR descriptions.
- Reduced Duplication: The 'autonomous-developer' and
  'semi-autonomous-developer' skills have been refactored to remove duplicated
  PR creation logic, now referencing the centralized 'pr-manager' skill.
- Dependency Diagram Updates: The 'skill_dependencies.puml' diagram has been
  updated to include the new 'pr-manager' skill and reflect the updated
  dependencies for 'autonomous-developer' and 'semi-autonomous-developer'.
#+end_quote

*** COMPLETED Fix valgrind errors in nightly                           :code:
    :LOGBOOK:
    CLOCK: [2026-01-14 Wed 09:00]--[2026-01-14 Wed 09:15] =>  0:15
    CLOCK: [2026-01-13 Tue 09:01]--[2026-01-13 Tue 09:39] =>  0:38
    CLOCK: [2026-01-12 Mon 11:31]--[2026-01-12 Mon 12:03] =>  0:32
    :END:

- lots of errors in eventing tests, but just for clang.
- 1 new possible leak for both clang and gcc.

*** COMPLETED Create security module                                   :code:
    :LOGBOOK:
    CLOCK: [2026-01-13 Tue 17:04]--[2026-01-13 Tue 18:27] =>  1:23
    :END:

#+begin_quote
This pull request establishes a dedicated ores.security component to consolidate
and manage all shared security primitives within the project. It systematically
extracts and relocates cryptographic operations and validation utilities from
existing modules like ores.iam and ores.connections. The refactoring also
introduces modern C++ practices, such as RAII for OpenSSL resources, to enhance
code reliability and maintainability. This change centralizes security logic,
making it easier to manage, audit, and extend in the future.

Highlights:

- New Component Creation: A new component, ores.security, has been introduced to
  centralize shared security primitives, promoting better organization and
  reusability of security-related code.
- Code Migration and Refactoring: Existing security code, including password
  management and encryption services, has been migrated from ores.iam and
  ores.connections into the new ores.security component. This involved renaming
  classes (e.g., password_manager to password_hasher, encryption_service to
  encryption) and their methods for clarity and consistency within the new
  structure.
- RAII Pattern for OpenSSL Contexts: The RAII (Resource Acquisition Is
  Initialization) pattern has been applied to OpenSSL cipher context management
  using std::unique_ptr with a custom deleter (EVP_CIPHER_CTX_free). This
  significantly improves memory safety by ensuring automatic resource cleanup,
  eliminating manual cleanup on error paths, and making the code more robust.
- CMake and Build System Updates: The CMake build system has been updated to
  include the new ores.security component and adjust dependencies in ores.cli,
  ores.connections, and ores.iam to link against the new security library.
#+end_quote

*** COMPLETED Prodigy should support multiple environments             :code:
    :LOGBOOK:
    CLOCK: [2026-01-13 Tue 10:41]--[2026-01-13 Tue 11:00] =>  0:19
    :END:

Now that we are running multiple claudes, qwens and geminis, it is often the
case that we need to run a service from two environments. At present we cannot
do this without a lot of effort. Ideally prodigy should just show all
environments in one buffer and we can choose what to run.

*** COMPLETED Add named environment sets                               :code:
    :LOGBOOK:
    CLOCK: [2026-01-13 Tue 21:15]--[2026-01-13 Tue 23:22] =>  2:07
    CLOCK: [2026-01-13 Tue 16:02]--[2026-01-13 Tue 17:03] =>  1:01
    CLOCK: [2026-01-13 Tue 12:56]--[2026-01-13 Tue 13:25] =>  0:29
    CLOCK: [2026-01-13 Tue 12:15]--[2026-01-13 Tue 12:55] =>  0:40
    :END:

At present we are having to supply the port name and host which is fine when we
have a single environment. However, because we have so many instances on the go
at the same time, it is a bit painful to have to remember ports for each
environment. Ideally we want a client side list of servers and the UI to go with
it.

We need environment management, where we can add new connections and new
folders. Folders can contain sub-folders. Environments can have names and tags.
User can add tags. Data is stored in local SQL Lite database. Passwords are
hashed.

Then in Qt we have an environment manager which has the UI for adding new
environments, etc. We should have a "test connection" button.

Notes:

- need to be able to export to JSON the data and import from JSON. Must have
  option to exclude usernames and or passwords.
- need to add commands to shell for this.

**** Core support

#+begin_quote
This pull request delivers a brand new ores.connections component, establishing
a robust system for managing server connection bookmarks. It encompasses a
well-defined domain model for organizing connections hierarchically and by tags,
backed by secure SQLite persistence. The component features AES-256-GCM
encryption for sensitive credentials and exposes a user-friendly service layer
for all management operations, all thoroughly validated with extensive unit
tests.

Highlights:

- New Component: ores.connections: Introduces a dedicated component for managing
  server connection bookmarks, providing a structured way to store and organize
  connection details.
- Domain Modeling: Defines core domain types including 'folder' for hierarchical
  organization, 'tag' for flexible categorization, 'server_environment' for
  connection details, and 'environment_tag' for many-to-many relationships,
  complete with JSON and table I/O support.
- Secure Persistence: Implements SQLite-based local persistence with a manually
  defined schema, including foreign keys and cascade delete rules, ensuring data
  integrity. Passwords are securely stored using AES-256-GCM encryption with
  PBKDF2 key derivation.
- High-Level API: Provides a 'connection_manager' service as a high-level API to
  interact with the connection data, abstracting away the underlying storage and
  encryption complexities.
- Comprehensive Testing: Includes a robust test suite with 79 test cases
  covering the domain objects, repository layer, and service layer, ensuring the
  reliability and correctness of the new component.
#+end_quote

**** Qt UI Support

#+begin_quote
This pull request significantly enhances the application's connection management
capabilities by introducing a dedicated Connection Browser. This new feature
allows users to securely store, organize, and manage their server connections
using a hierarchical structure and master password encryption. It streamlines
the login process by enabling pre-filled connection details and automatic
submission, while also providing robust tools for managing connection data and
ensuring a smoother user experience.

Highlights:

- Connection Browser MDI Window: Introduced a new MDI window for managing saved
  server connections, featuring a hierarchical tree view for folders and
  environments. This allows users to organize and access their connections
  efficiently before logging in.
- Secure Credential Management: Implemented master password support using
  AES-256-GCM encryption for securing saved server credentials. This includes
  features to create, change, and unlock access to encrypted passwords,
  enhancing security for stored connection details.
- CRUD Operations for Connections and Folders: Added comprehensive Create, Read,
  Update, and Delete (CRUD) functionalities for both connection folders and
  individual server environments within the Connection Browser, providing full
  control over saved configurations.
- Database Purge and Connection Handling: Included a 'Purge Database' action to
  reset all saved connections and folders. The system now also gracefully
  handles attempts to connect while already connected, prompting the user to
  disconnect first.
- Enhanced Login Experience: The Login Dialog now supports pre-filling
  connection details from saved entries and an auto-submit mode for seamless
  login when all credentials (including password) are available.
- Icon Guidelines Update: Updated the icon guidelines documentation to include
  new icons for 'Delete Dismiss', 'Key Multiple', and 'Server Link', reflecting
  the new functionalities.
#+end_quote

*** COMPLETED Rename risk to refdata                                   :code:
    :LOGBOOK:
    CLOCK: [2026-01-14 Wed 10:11]--[2026-01-14 Wed 10:25] =>  0:14
    CLOCK: [2026-01-14 Wed 09:41]--[2026-01-14 Wed 10:10] =>  0:29
    :END:

We need to split risk from ref data.

#+begin_quote
This pull request executes a significant refactoring effort by renaming a core
component from ores.risk to ores.refdata. The change aims to clarify the
component's role within the system, emphasizing its function in managing
reference data rather than performing risk calculations. The scope of this
change is broad, touching upon various layers of the application, from build
configurations and source code to documentation and architectural diagrams,
ensuring consistency and improved semantic clarity across the entire project.

Highlights:

- Component Rename: The ores.risk component has been comprehensively renamed to
  ores.refdata to better reflect its purpose as a reference data module for ORE
  (Open Source Risk Engine).
- Codebase-Wide Updates: This rename involved updating all include paths from
  ores.risk/ to ores.refdata/, changing the C++ namespace from ores::risk to
  ores::refdata, and modifying CMake targets (e.g., ores.risk.lib to
  ores.refdata.lib).
- Header Guard and File Renames: Header guards were updated from ORES_RISK_* to
  ORES_REFDATA_*, and risk_routes.cpp was renamed to refdata_routes.cpp in the
  HTTP server.
- Documentation and Diagram Synchronization: All PlantUML diagrams and various
  documentation files (e.g., sprint backlogs, skill definitions) were updated to
  correctly reference the new component name and aliases.
#+end_quote

*** COMPLETED Prefix database entities with component                  :code:
    :LOGBOOK:
    CLOCK: [2026-01-14 Wed 14:55]--[2026-01-14 Wed 15:33] =>  0:38
    CLOCK: [2026-01-14 Wed 14:30]--[2026-01-14 Wed 14:54] =>  0:24
    CLOCK: [2026-01-14 Wed 12:31]--[2026-01-14 Wed 13:05] =>  0:34
    CLOCK: [2026-01-14 Wed 10:26]--[2026-01-14 Wed 12:05] =>  1:39
    :END:

The database is becoming a bit confusing and we can't tell what entities belong
where. We need to prefix them by component, e.g. "accounts" becomes
"iam_accounts".

#+begin_quote
This pull request introduces a fundamental refactoring of the SQL database
schema by implementing a new, consistent naming convention across all entities.
The primary goal is to enhance the organization and readability of the database,
making it easier to understand the purpose and ownership of each object. This
change facilitates better integration with the C++ codebase and streamlines
future schema evolution by providing clear guidelines for naming new components.

Highlights:

- Consistent Naming Convention: All SQL tables, functions, triggers, and rules
  now adhere to a strict {component}_{entity}_{suffix} naming pattern, improving
  clarity and maintainability.
- Component Prefixes: New component prefixes (e.g., iam_, refdata_, assets_,
  telemetry_, geo_, variability_, utility_) have been introduced to align SQL
  entities with their corresponding C++ components.
- Standardized Suffixes: Entity suffixes like _tbl for tables, _fn for
  functions, _trg for triggers, _rule for rules, and various index suffixes
  (_idx, _uniq_idx, _gist_idx) are now consistently applied.
- C++ Code Alignment: Corresponding C++ entity files have been updated to
  reflect the new SQL table names in their tablename constants, ensuring
  synchronization between code and schema.
- Enhanced Documentation: The sql-schema-creator documentation has been
  significantly expanded to detail the new naming conventions, provide examples,
  and list existing entities by component, serving as a comprehensive guide for
  future development.
#+end_quote

*** COMPLETED Implement data quality                                   :code:
    :LOGBOOK:
    CLOCK: [2026-01-15 Thu 16:27]--[2026-01-15 Thu 18:08] =>  1:41
    CLOCK: [2026-01-15 Thu 15:50]--[2026-01-15 Thu 16:26] =>  0:36
    CLOCK: [2026-01-15 Thu 15:35]--[2026-01-15 Thu 15:50] =>  0:15
    CLOCK: [2026-01-15 Thu 14:30]--[2026-01-15 Thu 15:15] =>  0:45
    CLOCK: [2026-01-15 Thu 11:51]--[2026-01-15 Thu 14:01] =>  2:10
    CLOCK: [2026-01-15 Thu 11:31]--[2026-01-15 Thu 11:50] =>  0:19
    CLOCK: [2026-01-14 Wed 23:30]--[2026-01-15 Thu 00:37] =>  1:07
    CLOCK: [2026-01-14 Wed 17:03]--[2026-01-14 Wed 18:03] =>  1:00
    CLOCK: [2026-01-14 Wed 15:34]--[2026-01-14 Wed 17:03] =>  1:29
    CLOCK: [2026-01-13 Tue 13:38]--[2026-01-13 Tue 13:58] =>  0:20
    :END:

Originally we added seed data more or less haphazardly to the system. We now
need to clean this up. We need to have a user driven way to inject sample data
in the system, and the sample data must be curated carefully in order to
preserve it's lineage and provenience.

- all data must have a source, a date at which it was sourced and a licence (if
  available). We should record who sourced it (email, name).
- data should be marked as synthetic (generated), primary or derived. Unless
  there is a data quality classification.
- data should be organised into collections / data sets.
- all data should be stored in the database, we probably don't even need c++
  representation for it. There should be a stored procedure that copies data
  from sample tables to actual tables.

Notes:

- rename drop all script to drop schema.
- enum tables are using UUIDs. use the enum name instead.

**** Analysis from Qwen

***** Core Concepts

| Concept          | Description                                                                                                             |
|------------------+-------------------------------------------------------------------------------------------------------------------------|
| Dataset          | A logical collection of sample records (e.g., a set of trades, portfolios, or market data snapshots).                   |
| Record           | An individual row or entity within a dataset (optional granularity; lineage may be tracked at dataset or record level). |
| Provenance       | The origin of the data—external vendor, internal system, or synthetic generator.                                        |
| Classification   | Categorizes the nature of the data for compliance and usage control.                                                    |
| Lineage          | Directed acyclic graph (DAG) linking derived data to its upstream sources.                                              |
| Temporal Context | Captures both business time (as_of_date) and system time (ingestion_timestamp), supporting bi-temporal reasoning.       |
| Data Passport    | A self-contained metadata manifest (relational or document-based) that answers the 5 Ws of data governance.             |

**** Metadata Attributes

Each dataset (or optionally each record) must carry the following metadata
fields.

***** Provenance & Classification

| Field               | Type                      | Required                | Description                                                                                                 |
|---------------------+---------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------|
| source_system_id    | String (UUID or mnemonic) | ✅                     | Identifier of the originating system (e.g., "BLOOMBERG", "INTERNAL_RISK_ENGINE_v2").                        |
| data_classification | Enum                      | ✅                     | One of: REAL, SYNTHETIC, OBFUSCATED, HYBRID.                                                                |
| generation_method   | String / Enum             | ✅ if synthetic/hybrid | Describes how data was created (e.g., "MONTE_CARLO", "REPLAY_OF_20231004", "BOOTSTRAP_FROM_PROD_SNAPSHOT"). |
| business_context    | String                    | ✅                     | Human-readable purpose (e.g., "Testing FX Forward P&L attribution", "Demo for client onboarding").          |

***** B. Lineage & Derivation

| Field                    | Type          | Required                  | Description                                                                                       |
|--------------------------+---------------+---------------------------+---------------------------------------------------------------------------------------------------|
| upstream_derivation_id   | String (UUID) | ❌ (required if derived) | Points to the parent dataset or record ID from which this was derived.                            |
| transformation_logic_ref | URI / String  | ❌ (required if derived) | Reference to the logic that produced this data (e.g., Git commit hash, script name, workflow ID). |
| lineage_depth            | Integer       | ✅                       | Number of derivation steps from the original raw source (0 = raw/golden copy).                    |

***** C. Temporal Metadata

| Field               | Type            | Required | Description                                                                     |
|---------------------+-----------------+----------+---------------------------------------------------------------------------------|
| as_of_date          | Date            | ✅      | The business/market date the data represents (e.g., valuation date).            |
| ingestion_timestamp | Timestamp (UTC) | ✅      | When the data entered the sample repository.                                    |
| valid_from          | Timestamp (UTC) | ✅      | Start of validity interval (for time-sliced data).                              |
| valid_to            | Timestamp (UTC) | ✅      | End of validity interval (use far-future sentinel like 9999-12-31 for current). |

Note: valid_from/valid_to enable bi-temporal modeling—essential for replaying
historical states in trading systems.

Granularity Options

Choose one of the following approaches based on your use case.

| Option                 | Description                                                    | When to Use                                                                                            |
|------------------------+----------------------------------------------------------------+--------------------------------------------------------------------------------------------------------|
| Dataset-level metadata | One metadata record per dataset (e.g., per CSV file or table). | For bulk samples where all rows share the same origin and transformation.                              |
| Record-level metadata  | Each row carries its own metadata (via FK or embedded).        | When lineage varies per trade/portfolio (e.g., hybrid datasets mixing real and synthetic instruments). |

Data Passport Structure (Logical View)

Every sample dataset should be accompanied by a Data Passport manifest containing:

#+begin_src json
{
    "who": "",
    "what": {
        "classification": "<SYNTHETIC|REAL|...>",
        "generation_method": "<...>"
    },
    "where": {
        "upstream_derivation_id": "<UUID?>",
        "transformation_logic_ref": "<URI?>"
    },
    "when": {
        "as_of_date": "YYYY-MM-DD",
        "ingestion_timestamp": "ISO8601 UTC",
        "valid_from": "ISO8601 UTC",
        "valid_to": "ISO8601 UTC"
    },
    "why": ""
}
#+end_src

This can be stored as:

- A JSON/JSONB column in a relational table
- A standalone .passport.json file alongside Parquet/CSV
- A row in a metadata_registry table

Constraints & Validation Rules

- If data_classification ∈ {SYNTHETIC, HYBRID}, then generation_method must be
  non-null.
- If lineage_depth > 0, then upstream_derivation_id and transformation_logic_ref
  must be provided.
- valid_from ≤ valid_to
- as_of_date should align with valid_from/valid_to for consistency (e.g.,
  as_of_date ∈ [valid_from, valid_to]).

*** COMPLETED Improvements to connection browser                       :code:
    :LOGBOOK:
    CLOCK: [2026-01-15 Thu 21:25]--[2026-01-15 Thu 23:30] =>  2:05
    :END:

- can't drag and change order of folders.
- can't rename inline.
- We should validate the password to make sure it complies with password
  policies.
- add test connection to check the connection and credentials are correct.
- disable buttons are very bright, look enabled.
- connection browser should start with expanded tree.
- no tags support.
- login dialog should support connections from connection browser.

*** COMPLETED Modernise login dialog                                   :code:

#+begin_quote
This pull request significantly overhauls the user authentication experience by
introducing new, dark-themed login and signup dialogs. These dialogs are now
modeless MDI subwindows, improving workflow and integration within the
application. Key enhancements include automatic login after successful
registration, a dropdown for quick access to saved connections, and a refreshed
visual design.

Highlights:

- Modernized Login and Sign Up Dialogs: Introduced new dark-themed, modeless
  LoginDialog and SignUpDialog widgets, replacing older modal designs for a more
  integrated user experience.
- Streamlined User Registration: Implemented an auto-login feature that
  automatically authenticates a user after successful registration, reducing
  friction in the onboarding process.
- Enhanced Connection Management: Added a saved connections dropdown within the
  login dialog, allowing users to quickly select and connect to previously
  configured servers.
- MDI Subwindow Integration: The new login and sign-up dialogs are now presented
  as MDI subwindows, providing better window management and a more flexible UI
  layout.
#+end_quote

*** COMPLETED History should have a revert button                      :code:

*Rationale*: Implemented.

It should be possible to choose a given version and ask the system to revert the
currency to that version. It just makes an update with a new version to look
like that version. It should also be possible to open at version. It shows the
edit dialog in "read only mode" with the entity at that version. The tool bar
should indicate this. It should also have a revert in that dialog.

*** COMPLETED Disconnect shows reconnect message                       :code:
    :LOGBOOK:
    CLOCK: [2026-01-15 Thu 23:31]--[2026-01-16 Fri 01:55] =>  2:24
    :END:

*Rationale*: implemented.

When you click disconnect the status bar says "Reconnecting to server".

We can see but not edit. Also, test permissions to make sure they are being
enforced.

*** STARTED Refactor documentation                                    :infra:
    :LOGBOOK:
    CLOCK: [2026-01-13 Tue 15:01]--[2026-01-13 Tue 16:02] =>  1:01
    CLOCK: [2026-01-13 Tue 08:50]--[2026-01-13 Tue 09:00] =>  0:10
    CLOCK: [2026-01-12 Mon 21:55]--[2026-01-12 Mon 23:50] =>  1:55
    CLOCK: [2026-01-12 Mon 14:55]--[2026-01-12 Mon 17:52] =>  2:57
    CLOCK: [2026-01-12 Mon 12:04]--[2026-01-12 Mon 12:30] =>  0:26
    :END:

- split recipes per entity.
- create a top-level "domain" document per entity which cross-references
  recipes, component model, entity implementation files (including SQL), skills.
- recipes should belong to the component rather than being global.

**** Site fixes

#+begin_quote
This pull request primarily focuses on improving the stability and reliability
of the site build process by introducing robust error handling and preventing
debugger interruptions. Concurrently, it addresses and resolves critical broken
internal links within the project's documentation, enhancing the overall user
experience and information accessibility. Minor updates to agile sprint tracking
documentation are also included.

Highlights:

- Build Process Robustness: Implemented error handling for the org-publish-all
  function across all build scripts (.build-plan.el, .build-site.el,
  .build-skills.el) to gracefully catch and report build failures instead of
  crashing. Additionally, Emacs debugger and quit-on-error behaviors are
  suppressed during the build to ensure uninterrupted execution.
- Broken Link Fixes: Corrected several broken internal links within the
  doc/recipes/cli_recipes.org file by updating outdated UUIDs for the ORE Studio
  Geo and Telemetry Components, ensuring accurate navigation within the
  documentation.
- Documentation Updates: Updated clock table entries and summaries in
  doc/agile/v0/sprint_backlog_09.org to reflect recent work, and added new notes
  and observations related to the 'Clean up seed data' task.
#+end_quote

*** STARTED Add UI support for data sets                               :code:
    :LOGBOOK:
    CLOCK: [2026-01-18 Sun 23:45]
    CLOCK: [2026-01-18 Sun 19:01]--[2026-01-18 Sun 19:20] =>  0:19
    CLOCK: [2026-01-18 Sun 18:35]--[2026-01-18 Sun 19:01] =>  0:26
    CLOCK: [2026-01-18 Sun 16:54]--[2026-01-18 Sun 17:20] =>  0:26
    CLOCK: [2026-01-18 Sun 16:01]--[2026-01-18 Sun 16:54] =>  0:53
    CLOCK: [2026-01-18 Sun 10:13]--[2026-01-18 Sun 11:20] =>  1:07
    CLOCK: [2026-01-18 Sun 08:40]--[2026-01-18 Sun 10:12] =>  1:32
    CLOCK: [2026-01-17 Sat 22:28]--[2026-01-17 Sat 22:47] =>  0:19
    CLOCK: [2026-01-17 Sat 18:10]--[2026-01-17 Sat 22:27] =>  4:17
    CLOCK: [2026-01-16 Fri 16:27]--[2026-01-16 Fri 17:23] =>  0:56
    CLOCK: [2026-01-16 Fri 15:20]--[2026-01-16 Fri 16:26] =>  1:06
    CLOCK: [2026-01-16 Fri 14:07]--[2026-01-16 Fri 15:01] =>  0:54
    CLOCK: [2026-01-16 Fri 13:02]--[2026-01-16 Fri 14:06] =>  1:04
    CLOCK: [2026-01-16 Fri 11:37]--[2026-01-16 Fri 11:45] =>  0:08
    :END:

We did all of the database work but we cannot visualise the datasets.

Notes

- we need to check this issue across all triggers: inconsistent versioning.
  Done.

**** UI Work

Gemini:

#+begin_quote
Data Librarian" is an excellent name. In financial institutions, the role of a
Data Librarian (or Data Steward) is exactly what you are describing: someone who
curates, catalogs, and ensures the provenance of "information books" (datasets)
within the "library" (the trading system).

To manage the types you've created, the UI needs to handle Hierarchical
Relationships (Domain > Subject Area) and Metadata Tagging (Dimensions & Coding
Schemes).

1. UI Conceptual Layout: The interface should be split into three functional
   zones:
   - The Stacks (Navigation): A sidebar for the hierarchy (Domain > Subject Area
     > Catalog).
   - The Registry (Workspace): The main area to define Dimensions (Origin,
     Nature, Treatment) and Methodologies.
   - The Archive (Dataset View): The specific view for individual Datasets and
     their Lineage.
2. Dashboard Component: "The Collection Manager"This is where you would manage
   your Dimensions and Coding Schemes.

[[./mock_ui_data_quality.png]]

- Detailed View: "Dataset Accession Card" When you create or edit a Dataset
  (UUID), the UI should present a "card" that forces the user to satisfy the
  Lineage requirements you defined.

  - Name: GBP_USD_STRESS_SCENARIO_01
  - UUID: 550e8400-e29b-41d4-a716-446655440000
    Catalog: Fixed Income Sample
    Sets***

- Classification Section (The Dimensions)
  - Origin: Vendor (Bloomberg)
  - Nature: Hybrid (Real + Synthetic)
  - Treatment: Normalized

- Lineage & Methodology (The "Trace")
  - Methodology: Monte_Carlo_Volatility_Shift_v2 [View Code]
  - Upstream Source: REF_CCY_BASE_LIVE
  - Derivation Logic: "Applied 20% haircut to base spot rates for stress
    testing."

4. UI Nomenclature Suggestions: Since you are calling the tool Data Librarian,
   lean into the metaphor to make the UI intuitive:
   - Librarian: Catalog. The CollectionA curated group of related works.
   - Dataset: The Volume. An individual instance of data.
   - Methodology: The Formula. How the data was "written" or derived.
   - Origin/Nature: Attributes. The "edition" details of the data.
   - Coding Scheme: The Index. How the data is filed (ISIN, RIC, etc.).

5. Metadata Sidecar Logic: In your UI, you should have a "Lineage Visualizer"
   tab for every Dataset. This is crucial for your requirement of "adequately
   traced to a source." It should show a flow diagram:

   Source (Origin) -> Methodology (Treatment) -> Target (Dataset)
#+end_quote

Grok:

#+begin_src ascii
┌──────────────────────────────────────────────────────────────────────────────────────────────┐
│  Data Librarian                                      v0.9.2  [Dark Theme]  User: M.Craveiro  │
├───────────────┬──────────────────────────────────────────────────────────────────────────────┤
│  Stacks       │  Collection Manager  (The Registry)                                          │
│  (Navigation) │                                                                              │
│               │  ┌──────────────────────────────────────────────────────────────┐            │
│  ▼ Domains    │  │ Dimensions & Coding Schemes Catalog                     ▼    │            │
│               │  │                                                              │            │
│  ┌──────────┐ │  │  [ + New Dimension ]   [ + New Coding Scheme ]               │            │
│  │ Fixed    │ │  ├──────────────────────────────────────────────────────────────┤            │
│  │ Income   │ │  │ Name               │ Type      │ Scope          │ Status     │            │
│  │          │ │  │────────────────────┼───────────┼────────────────┼────────────│            │
│  │  ▼       │ │  │ Origin             │ Enum      │ Global         │ Locked     │            │
│  │ Subject  │ │  │   • Vendor         │           │                │            │            │
│  │ Areas    │ │  │   • Internal       │           │                │            │            │
│  │          │ │  │   • Market         │           │                │            │            │
│  │  ┌─────┐ │ │  │ Nature             │ Enum      │ Global         │ Active     │            │
│  │  │Rates│ │ │  │   • Raw            │           │                │            │            │
│  │  │     │ │ │  │   • Normalized     │           │                │            │            │
│  │  │     │ │ │  │   • Hybrid         │           │                │            │            │
│  │  │Stres│ │ │  │ Treatment          │ Enum      │ Global         │ Active     │            │
│  │  │Scen.│ │ │  │   • As-is          │           │                │            │            │
│  │  └─────┘ │ │  │   • Cleaned        │           │                │            │            │
│  │          │ │  │   • Normalized     │           │                │            │            │
│  │  Catalog │ │  │ RIC                │ Coding    │ Security       │ Active     │            │
│  │   ▼      │ │  │ ISIN               │ Coding    │ Security       │ Active     │            │
│  │ Fixed    │ │  │ Bloomberg Ticker   │ Coding    │ Security       │ Deprecated │            │
│  │ Income   │ │  └──────────────────────────────────────────────────────────────┘            │
│  │ Sample   │ │                                                                              │
│  └──────────┘ │                                                                              │
│               │  ┌───────────────────────────────┐   ┌───────────────────────────────┐       │
│               │  │ Recently Accessed Volumes     │   │ Recently Modified Formulas    │       │
│               │  │ • GBP_USD_STRESS_SCENARIO_01  │   │ • Monte_Carlo_Volatility_Shift│       │
│               │  │ • EUR_IR_CURVE_2025_Q4        │   │   _v2                         │       │
│               │  │ • US_TREASURY_PAR_CURVE_D     │   │ • FX_FWD_POINT_BOOTSTRAP_v3   │       │
│               │  └───────────────────────────────┘   └───────────────────────────────┘       │
├───────────────┴──────────────────────────────────────────────────────────────────────────────┤
│  Dataset Accession Card – GBP_USD_STRESS_SCENARIO_01                                 [Close] │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│ UUID:    550e8400-e29b-41d4-a716-446655440000                                         [Copy] │
│ Catalog: Fixed Income Sample                                                                 │
│ Volume Name: GBP_USD_STRESS_SCENARIO_01                                                      │
│                                                                                              │
│ ┌─────────────────────────────┐ ┌─────────────────────────────┐ ┌──────────────────────┐     │
│ │ Classification (Dimensions) │ │        The Trace            │ │     Metadata         │     │
│ │                             │ │   (Lineage & Methodology)   │ │     Sidecar          │     │
│ │ Origin    : Vendor          │ │ Methodology                 │ │ Version      : 1.2   │     │
│ │           : Bloomberg       │ │   Monte_Carlo_Volatility_   │ │ Status       : Prod  │     │
│ │ Nature    : Hybrid          │ │   Shift_v2       [View Code]│ │ Owner     : Rates    │     │
│ │           : Real + Synthetic│ │                             │ │ Last Modified: 2026- │     │
│ │ Treatment : Normalized      │ │ Upstream Sources            │ │ 01-14 11:42          │     │
│ │                             │ │   • REF_CCY_BASE_LIVE       │ └──────────────────────┘     │
│ └─────────────────────────────┘ │   • USD_IR_CURVE_SOY_2025   │                              │
│                                 │                             │                              │
│                                 │ Derivation Summary          │                              │
│                                 │ "Applied 20% haircut to     │                              │
│                                 │  base spot rates for stress │                              │
│                                 │  testing purposes"          │                              │
│                                 └─────────────────────────────┘                              │
├──────────────────────────────────────────────────────────────────────────────────────────────┤
│ Lineage Visualizer (Flow)                                             [Export SVG / PNG]     │
│                                                                                              │
│     ┌────────────────────┐          ┌────────────────────────────┐         ┌───────────────┐ │
│     │   Bloomberg        │          │ Monte_Carlo_Volatility_    │         │ GBP_USD_STRESS│ │
│     │ REF_CCY_BASE_LIVE  │ ────────►│ Shift_v2                   │ ───────►│ _SCENARIO_01  │ │
│     │ (Live spot rates)  │   1:1    │ 20% volatility haircut     │  1:1    │ (Stress vol   │ │
│     └────────────────────┘          └────────────────────────────┘         │ surface)      │ │
│                                                                            └───────────────┘ │
│                                                                                              │
│ [ + Add upstream ]     [ Show full provenance tree ]     [ Show data quality gates ]         │
└──────────────────────────────────────────────────────────────────────────────────────────────┘
#+end_src

HF:

[[./mock_data_librarian_hf.png]]

**** Subject areas

#+begin_quote
This pull request significantly enhances the application's data organization
capabilities by introducing a new module for managing Subject Areas. It provides
a structured way to define and track subject areas, complete with dedicated user
interfaces for detailed viewing, editing, and a robust version history system.
This integration improves the overall data governance and traceability within
the application.

Highlights:

- New Subject Area Management Feature: Introduced a comprehensive system for
  managing 'Subject Areas', allowing users to create, view, edit, delete, and
  track the history of subject areas within data domains.
- Dedicated Controller and UI Components: Added a new SubjectAreaController to
  manage the lifecycle of subject area-related windows, along with
  SubjectAreaDetailDialog for individual subject area operations and
  SubjectAreaHistoryDialog for viewing version history and changes.
- Main Window Integration: The main application window (MainWindow) has been
  updated to include and initialize the SubjectAreaController, connect its
  signals to the status bar, and enable a new 'Subject Areas' action in the
  menu, making the feature accessible upon successful login.
- Version History and Revert Functionality: The SubjectAreaHistoryDialog
  provides a detailed view of a subject area's changes over time, including who
  made the change, when, and a commentary. It also enables opening read-only
  versions and reverting to previous states.
#+end_quote

**** Address review comments

#+begin_quote
This pull request significantly enhances the system's test suite and
architectural design. It improves the quality and determinism of Data Quality
(DQ) domain type tests through more thorough assertions and fixed time point
generation. Crucially, it addresses and resolves a circular dependency between
the DQ and Identity and Access Management (IAM) components by centralizing
shared constants, leading to a more robust and maintainable codebase. The
changes also include updates to internal documentation and system diagrams to
reflect these structural improvements.

Highlights:

- Improved Test Assertions for DQ Domain Types: Enhanced test assertions for
  Data Quality (DQ) domain types, including explicit checks for audit fields
  like 'recorded_by' and 'change_commentary', as well as verification of table
  headers and field values in table conversion tests.
- Deterministic Time Values in Tests: Introduced a 'make_timepoint' helper
  function to generate fixed, deterministic time values for
  'std::chrono::system_clock::time_point' fields in tests, ensuring
  repeatability and reliability.
- Updated Skill Documentation: Added new guidance to skill documentation for
  binary protocol developers and domain type creators, covering best practices
  for passing authorization services to message handlers and comprehensive
  domain type testing.
- Circular Dependency Resolution: Resolved a circular dependency between the
  'ores.dq' and 'ores.iam' components by relocating the
  'change_reason_constants.hpp' header to the more foundational 'ores.database'
  module, improving architectural modularity.
- System Model and CMake Dependency Updates: Synchronized the system model
  diagrams ('ores.puml') and CMake configurations across various projects
  ('ores.cli', 'ores.comms.shell', 'ores.dq', 'ores.iam') to accurately reflect
  the updated component dependencies, particularly concerning the DQ subsystem
  and the centralized change reason constants.
#+end_quote

**** Messaging

#+begin_quote
This pull request significantly expands the ORE Studio binary protocol by
integrating a new Data Quality (DQ) subsystem. This enhancement provides robust
messaging capabilities for managing a wide array of data quality entities,
centralizing related functionalities. The changes also involve a re-architecture
of existing change management features, moving them into the new DQ domain,
which necessitates a protocol version update to ensure compatibility and reflect
the breaking nature of this re-organization.

Highlights:

- New Data Quality (DQ) Subsystem: A dedicated subsystem (0x6000-0x6FFF) has
  been introduced to the ORE Studio binary protocol to manage Data Quality
  related messages and entities.
- Comprehensive DQ Messaging Protocols: New messaging protocols are added to
  support CRUD (Create, Read, Update, Delete) and history operations for various
  data quality entities, including catalogs, data domains, subject areas,
  datasets, methodologies, coding schemes, and dimensions (nature, origin,
  treatment).
- Migration of Change Management Messages: Existing change management messages
  (for change reasons and categories) have been relocated from the IAM subsystem
  (0x2050-0x2061) to the new DQ subsystem (0x6070-0x6081). This is a breaking
  change in message type IDs.
- Binary Protocol Version Update: The ORE Studio binary protocol version has
  been incremented from 21.3 to 22.0 to reflect the introduction of the new DQ
  subsystem and the breaking changes associated with message re-organization.
- Dedicated DQ Message Handling: A new dq_message_handler and its registrar have
  been implemented to process all incoming messages within the new Data Quality
  subsystem, ensuring proper routing and business logic execution for DQ-related
  requests.
#+end_quote

**** Data catalog work

#+begin_quote
This pull request significantly enhances the data quality management framework
by introducing a new dq_catalog entity, which serves as a high-level grouping
mechanism for related datasets. Concurrently, it undertakes a major refactoring
of the Entity-Relationship (ER) diagram, standardizing table naming conventions,
reorganizing entities into component-based packages, and incorporating a
comprehensive set of data quality entities, as well as new telemetry and
geolocation domains. These changes aim to improve data organization, clarity,
and the overall structural integrity of the database schema.

Highlights:

- New Data Quality Catalog Entity: Introduced dq_catalog_tbl as a top-level
  grouping mechanism for datasets within the data quality domain, allowing for
  logical organization of related data.
- ER Diagram Standardization and Expansion: The Entity-Relationship (ER) diagram
  (ores_schema.puml) has undergone a major overhaul, implementing a consistent
  <component>_<entity>_tbl naming convention across all tables and reorganizing
  packages by component prefix (e.g., iam, assets, refdata, variability, dq,
  telemetry, geo).
- Comprehensive Data Quality Entities: All missing Data Quality (DQ) entities,
  including dimensions (nature, origin, treatment), methodology, dataset, and
  various artefact tables, have been added to the ER diagram, enhancing the data
  quality framework.
- Integration of Telemetry and Geolocation Domains: New telemetry and
  geolocation domains, along with their respective tables (telemetry_logs_tbl
  and geo_ip2country_tbl), have been integrated into the ER diagram.
- Dataset Assignment to Catalogs: Initial population scripts for dq_catalog_tbl
  have been added, creating 'ISO Standards' and 'Cryptocurrency' catalogs, and
  existing datasets are now assigned to these new catalogs.
#+end_quote

**** Domain types

#+begin_quote
This pull request significantly expands the data quality (DQ) infrastructure by
introducing a comprehensive set of domain types designed to manage and track
data integrity. It establishes foundational structures for organizing data,
defining classification schemes, and documenting processing methodologies. The
integration of FpML reference data further enriches the system's ability to
handle complex financial data, while new data generators streamline testing
workflows. These changes collectively enhance the robustness and traceability of
data within the system.

Highlights:

- New Data Quality Domain Types: Introduced 10 new domain types for Data Quality
  (DQ) database entities, including data_domain, subject_area, catalog,
  coding_scheme_authority_type, coding_scheme, origin_dimension,
  nature_dimension, treatment_dimension, methodology, and dataset. These types
  support JSON I/O and table I/O.
- FpML Reference Data Integration: Incorporated FpML non-ISO currencies,
  business centers, and business processes reference data. This includes new XML
  data files and corresponding SQL population scripts to integrate these
  financial standards into the DQ system.
- Synthetic Data Generators: Added test data generators for all new DQ domain
  types using faker-cxx, facilitating the creation of synthetic data for testing
  and development purposes.
- Date Serialization Support: Implemented a year_month_day reflector to enable
  proper serialization of std::chrono::year_month_day objects with reflect-cpp,
  ensuring consistent date handling.
- Enhanced Data Quality Documentation: Expanded documentation for data quality
  concepts, including the 'DQ 6' standard dimensions (Accuracy, Completeness,
  Consistency, Timeliness, Validity, Uniqueness) and relevant industry bodies
  like DAMA-DMBOK and TOGAF.
#+end_quote

*** STARTED Implement party related entities at database level         :code:
    :LOGBOOK:
    CLOCK: [2026-01-16 Fri 11:16]--[2026-01-16 Fri 11:36] =>  0:20
    :END:

The first step of this work is to get the entities to work at the database
schema level.

**** Table Structure: party

| Field Name           | Data Type   | Constraints     | Commentary                                                                                        |
|----------------------+-------------+-----------------+---------------------------------------------------------------------------------------------------|
| =party_id=           | Integer     | PK, Auto-Inc    | Internal surrogate key for database performance and foreign key stability.                        |
| =tenant_id=          | Integer     | FK (tenant)     | The "Owner" of this record. Ensures GigaBank's client list isn't visible to AlphaHedge.           |
| =party_name=         | String(255) | Not Null        | The full legal name of the entity (e.g., "Barclays Bank PLC").                                    |
| =short_name=         | String(50)  | Unique          | A mnemonic or "Ticker" style name used for quick UI displays (e.g., "BARC-LDN").                  |
| =lei=                | String(20)  | Unique/Null     | The ISO 17442 Legal Entity Identifier. Critical for regulatory reporting and GLEIF integration.   |
| =is_internal=        | Boolean     | Default: False  | Flag: If TRUE, this party represents a branch or entity belonging to the Tenant (The Bank).       |
| =party_type_id=      | Integer     | FK (scheme)     | Categorizes the entity: Bank, Hedge Fund, Corporate, Central Bank, or Exchange.                   |
| =postal_address=     | Text        |                 | Used for generating legal confirmations and settlement instructions.                              |
| =business_center_id= | Integer     | FK (scheme)     | Links to an FpML Business Center (e.g., GBLO, USNY). Determines holiday calendars for settlement. |
| =status=             | Enum        | Active/Inactive | Controls whether trades can be booked against this entity.                                        |
| =created_at=         | Timestamp   |                 | Audit trail for when the entity was onboarded.                                                    |

*** Add a "is alive" message                                           :code:

We need to brainstorm this. At present we can only tell if a server is there or
not by connecting. It would be nice to give some visual indicator to the user
that the server is not up as soon as the user types the host. this may not be a
good idea.

Notes:

- could tell client if registration / sign-up is supported.

*** Add UI for bootstrapping from Qt                                   :code:

At present we need to go to the shell to bootstrap. It would be easier if we
just had a button in Qt for this.

*** Add schemes for countries and currencies                           :code:

We should have data lineage where possible. There should be a table telling us
where we sourced countries, currencies etc. Even permissions/roles, if there are
Qany specs on this. There should be an entry for "internal".

*** Change reason and categories need permissions                      :code:

We need some very specific permissions as these are reg-sensitive.

*** Change reason not requested on delete                              :code:

At present you can delete entities without providing a change reason.

*** Remember dialog sizes and positions                                :code:

At present we need to resize dialogs frequently. We should write this to
QSettings.

*** Add party scheme entity                                            :code:

The following is the analysis for adding support to party schemes.

List of schemes to seed in SQL:

- party scheme: LEI
- Name: Legal Entity Identifier
- URI: http://www.fpml.org/coding-scheme/external/iso17442
- Description: Legal Entity Identifier (ISO 17442, 20-char alphanumeric). Global
  standard for legal entities.

- party scheme: BIC
- Name: Business Identifier Code
- URI: http://www.fpml.org/coding-scheme/external/iso9362
- Description: Business Identifier Code (SWIFT/BIC, ISO 9362). Used for banks
  and financial institutions.

- party scheme: MIC
- Name: Market Identifier Code
- URI: http://www.fpml.org/coding-scheme/external/iso10383
- Description: Market Identifier Code (ISO 10383). Identifies trading venues
  (e.g., XNYS, XLON). Note: Technically a venue, but often linked to party
  context in trade reports.

- party scheme: CEDB
- Name: CFTC Entity Directory
- URI: http://www.fpml.org/coding-scheme/external/cedb
- Description: CFTC Entity Directory (US-specific). Used in CFTC swap data
  reporting for non-LEI entities.

- party scheme: Natural Person
- Name: Natural Person
- URI: http://www.fpml.org/coding-scheme/external/person-id
- Description: Generic identifier for individuals (e.g., employee ID, trader
  ID). Not standardized; value interpreted contextually.

- party scheme: National ID
- Name: National Identifier
- URI: http://www.fpml.org/coding-scheme/extern/national-id
- Description: National identifiers (e.g., passport number, tax ID, SIREN, ORI,
  national ID card). Covers MiFID II “client identification” requirements.

- party scheme: Internal
- name: Internal
- URI: http://www.fpml.org/coding-scheme/external/party-id-internal
- Description: Proprietary/internal system identifiers (e.g., client ID in your
  OMS, CRM, or clearing system).

- party scheme: ACER Code
- name: EU Agency for Energy Regulation
- URI: http://www.fpml.org/coding-scheme/external/acer-code
- Description: ACER (EU Agency for Energy Regulation) code. Required for REMIT
  reporting by non-LEI energy market participants. Officially supported in FpML
  energy extensions.

- party scheme: DTCC Participant ID
- Name: Depository Trust & Clearing Corporation Participant ID
- URI:
- Description: DTCC Participant ID: A unique numeric identifier (typically 4–6
  digits) assigned by the Depository Trust & Clearing Corporation (DTCC) to
  member firms authorized to participate in U.S. clearing and settlement
  systems, including DTC, NSCC, and FICC. Used in post-trade processing, trade
  reporting, and regulatory submissions in U.S. capital markets.

- party scheme: AII / MPID
- Name: Alternative Trading System (ATS) Identification Indicator / Market Participant ID
- Description: Commonly referred to as MPID – Market Participant Identifier; AII
  stands for ATS Identification Indicator, a term used in FINRA contexts. A
  four-character alphanumeric code assigned by FINRA (Financial Industry
  Regulatory Authority) to broker-dealers and alternative trading systems (ATSs)
  operating in U.S. equities markets. Used to identify the originating or
  executing firm in trade reports (e.g., in OATS, TRACE, or consolidated tape
  reporting). MPIDs are required for all SEC-registered trading venues and
  participants in U.S. equity and options markets.

*** Add a organisation type scheme entity                              :code:

Indicates a type of organization.

- Obtained on 2016-06-13
- Version 2-0
- URL: http://www.fpml.org/coding-scheme/organization-type-2-0.xml

- Code: MSP
- Name: Major Swap Participant
- Description: A significant participant in the swaps market, for example as
  defined by the Dodd-Frank Act.

- Code: NaturalPerson
- Name: Natural Person
- Description: A human being.

- Code: non-SD/MSP
- Name: Non Swap Dealer or Major Swap Participant
- Description: A firm that is neither a swap dealer nor a major swaps participant under the Dodd-Frank Act.

- Code: SD
- Name: Swap Dealer
- Description:   Registered swap dealer.

*** Add party entity                                                   :code:

Party analysis.

| Field             | Type | Description                                | Foreign Key Reference      |
|-------------------+------+--------------------------------------------+----------------------------|
| party_id          | UUID | Primary key (globally unique identifier)   | —                          |
| full_name         | TEXT | Legal or registered name                   | —                          |
| short code        | TEXT | Short code for the party.                  |                            |
| organization_type | INT  | Type of organization                       | → organization_type_scheme |
| parent_party_id   | INT  | References parent party (self-referencing) | → party_id (nullable)      |

*** Add party identifier entity                                        :code:

Allows a party to have multiple external identifiers (e.g., LEI, BIC).

| Field       | Type | Description                               | Foreign Key Reference |
|-------------+------+-------------------------------------------+-----------------------|
| party_id    | UUID | References party.party_id                 | → party               |
| id_value    | TEXT | Identifier value, e.g., "549300..."       | —                     |
| id_scheme   | TEXT | Scheme defining identifier type, e.g. LEI | → party_id_scheme     |
| Description | TEXT | Additional information about the party    |                       |

Primary key: composite (party_id, id_scheme)

*** Add business centre scheme entity                                  :code:

The following is the analysis for adding support to party schemes.

Note: add a foreign key to the country table, which may be null in some cases.

The coding-scheme accepts a 4 character code of the real geographical business
calendar location or FpML format of the rate publication calendar. While the 4
character codes of the business calendar location are implicitly locatable and
used for identifying a bad business day for the purpose of payment and rate
calculation day adjustments, the rate publication calendar codes are used in the
context of the fixing day offsets. The 4 character codes are based on the
business calendar location some of which based on the ISO country code or
exchange code, or some other codes. Additional business day calendar location
codes could be built according to the following rules: the first two characters
represent the ISO 3166 country code [https://www.iso.org/obp/ui/#search/code/],
the next two characters represent either a) the first two letters of the
location, if the location name is one word, b) the first letter of the first
word followed by the first letter of the second word, if the location name
consists of at least two words. Note: for creating new city codes for US and
Canada: the two-letter combinations used in postal US states
(http://pe.usps.gov/text/pub28/28apb.htm ) and Canadian provinces
(http://www.canadapost.ca/tools/pg/manual/PGaddress-e.asp) abbreviations cannot
be utilized (e.g. the code for Denver, United States is USDN and not USDE,
because of the DE is the abbreviation for Delaware state ). Exchange codes could
be added based on the ISO 10383 MIC code
[https://www.iso20022.org/sites/default/files/ISO10383_MIC/ISO10383_MIC.xls]
according to the following rules: 1. it would be the acronym of the MIC. If
acronym is not available, 2. it would be the MIC code. If the MIC code starts
with an 'X', 3. the FpML AWG will compose the code. 'Publication Calendar Day',
per 2021 ISDA Interest Rate Derivatives Definitions, means, in respect of a
benchmark, any day on which the Administrator is due to publish the rate for
such benchmark pursuant to its publication calendar, as updated from time to
time. FpML format of the rate publication calendar. The construct: CCY-[short
codes to identify the publisher], e.g. GBP-ICESWAP. The FpML XAPWG will compose
the code.

- Obtained on 2025-04-25
- Version 9-4
- URL: http://www.fpml.org/coding-scheme/business-center-9-4.xml

- Code: The unique string/code identifying the business center, usually a
  4-character code based on a 2-character ISO country code and a 2 character
  code for the city, but with exceptions for special cases such as index
  publication calendars, as described above.

- Code: AEAB
- Description: Abu Dhabi, Business Day (as defined in 2021 ISDA Definitions
  Section 2.1.10 (ii))

- Code: AEAD
- Description: Abu Dhabi, Settlement Day (as defined in 2021 ISDA Definitions
  Section 2.1.10 (i))

- Code: AEDU
- Description: Dubai, United Arab Emirates

- Code: AMYE
- Description: Yerevan, Armenia

- Code: AOLU
- Description: Luanda, Angola

- Code: ARBA
- Description: Buenos Aires, Argentina

- Code: ATVI
- Description: Vienna, Austria

- Code: AUAD
- Description: Adelaide, Australia

- Code: AUBR
- Description: Brisbane, Australia

- Code: AUCA
- Description: Canberra, Australia

- Code: AUDA
- Description: Darwin, Australia

- Code: AUME
- Description: Melbourne, Australia

- Code: AUPE
- Description: Perth, Australia

- Code: AUSY
- Description: Sydney, Australia

- Code: AZBA
- Description: Baku, Azerbaijan

- Code: BBBR
- Description: Bridgetown, Barbados

- Code: BDDH
- Description: Dhaka, Bangladesh

- Code: BEBR
- Description: Brussels, Belgium

- Code: BGSO
- Description: Sofia, Bulgaria

- Code: BHMA
- Description: Manama, Bahrain

- Code: BMHA
- Description: Hamilton, Bermuda

- Code: BNBS
- Description: Bandar Seri Begawan, Brunei

- Code: BOLP
- Description: La Paz, Bolivia

- Code: BRBD
- Description: Brazil Business Day. This means a business day in all of Sao
  Paulo, Rio de Janeiro or Brasilia not otherwise declared as a financial market
  holiday by the Bolsa de Mercadorias &amp; Futuros (BM&F). BRBD should not be
  used for setting fixing time, instead the city centers (e.g. BRBR, BRRJ, BRSP)
  should be used, because they are locatable places.

- Code: BSNA
- Description: Nassau, Bahamas

- Code: BWGA
- Description: Gaborone, Botswana

- Code: BYMI
- Description: Minsk, Belarus

- Code: CACL
- Description: Calgary, Canada

- Code: Covers
- Description: all New Brunswick province.

- Code: CAFR
- Description: Fredericton, Canada.

- Code: CAMO
- Description: Montreal, Canada

- Code: CAOT
- Description: Ottawa, Canada

- Code: CATO
- Description: Toronto, Canada

- Code: CAVA
- Description: Vancouver, Canada

- Code: CAWI
- Description: Winnipeg, Canada

- Code: CHBA
- Description: Basel, Switzerland

- Code: CHGE
- Description: Geneva, Switzerland

- Code: CHZU
- Description: Zurich, Switzerland

- Code: CIAB
- Description: Abidjan, Cote d'Ivoire

- Code: CLSA
- Description: Santiago, Chile

- Code: CMYA
- Description: Yaounde, Cameroon

- Code: CNBE
- Description: Beijing, China

- Code: CNSH
- Description: Shanghai, China

- Code: COBO
- Description: Bogota, Colombia

- Code: CRSJ
- Description: San Jose, Costa Rica

- Code: CWWI
- Description: Willemstad, Curacao

- Code: CYNI
- Description: Nicosia, Cyprus

- Code: CZPR
- Description: Prague, Czech Republic

- Code: DECO
- Description: Cologne, Germany

- Code: DEDU
- Description: Dusseldorf, Germany

- Code: DEFR
- Description: Frankfurt, Germany

- Code: DEHA
- Description: Hannover, Germany

- Code: DEHH
- Description: Hamburg, Germany

- Code: DELE
- Description: Leipzig, Germany

- Code: DEMA
- Description: Mainz, Germany

- Code: DEMU
- Description: Munich, Germany

- Code: DEST
- Description: Stuttgart, Germany

- Code: DKCO
- Description: Copenhagen, Denmark

- Code: DOSD
- Description: Santo Domingo, Dominican Republic

- Code: DZAL
- Description: Algiers, Algeria

- Code: ECGU
- Description: Guayaquil, Ecuador

- Code: EETA
- Description: Tallinn, Estonia

- Code: EGCA
- Description: Cairo, Egypt

- Code: ESAS
- Description: ESAS Settlement Day (as defined in 2006 ISDA Definitions Section
  7.1 and Supplement Number 15 to the 2000 ISDA Definitions)

- Code: ESBA
- Description: Barcelona, Spain

- Code: ESMA
- Description: Madrid, Spain

- Code: ESSS
- Description: San Sebastian, Spain

- Code: ETAA
- Description: Addis Ababa, Ethiopia

- Code: EUR
- Description: -ICESWAP Publication dates for ICE Swap rates based on
  EUR-EURIBOR rates

- Code: EUTA
- Description: TARGET Settlement Day

- Code: FIHE
- Description: Helsinki, Finland

- Code: FRPA
- Description: Paris, France

- Code: GBED
- Description: Edinburgh, Scotland

- Code: GBLO
- Description: London, United Kingdom

- Code: GBP
- Description: -ICESWAP Publication dates for GBP ICE Swap rates

- Code: GETB
- Description: Tbilisi, Georgia

- Code: GGSP
- Description: Saint Peter Port, Guernsey

- Code: GHAC
- Description: Accra, Ghana

- Code: GIGI
- Description: Gibraltar, Gibraltar

- Code: GMBA
- Description: Banjul, Gambia

- Code: GNCO
- Description: Conakry, Guinea

- Code: GRAT
- Description: Athens, Greece

- Code: GTGC
- Description: Guatemala City, Guatemala

- Code: HKHK
- Description: Hong Kong, Hong Kong

- Code: HNTE
- Description: Tegucigalpa, Honduras

- Code: HRZA
- Description: Zagreb, Republic of Croatia

- Code: HUBU
- Description: Budapest, Hungary

- Code: IDJA
- Description: Jakarta, Indonesia

- Code: IEDU
- Description: Dublin, Ireland

- Code: ILJE
- Description: Jerusalem, Israel

- Code: ILS
- Description: -SHIR Publication dates of the ILS-SHIR index.

- Code: ILS
- Description: -TELBOR Publication dates of the ILS-TELBOR index.

- Code: ILTA
- Description: Tel Aviv, Israel

- Code: INAH
- Description: Ahmedabad, India

- Code: INBA
- Description: Bangalore, India

- Code: INCH
- Description: Chennai, India

- Code: INHY
- Description: Hyderabad, India

- Code: INKO
- Description: Kolkata, India

- Code: INMU
- Description: Mumbai, India

- Code: INND
- Description: New Delhi, India

- Code: IQBA
- Description: Baghdad, Iraq

- Code: IRTE
- Description: Teheran, Iran

- Code: ISRE
- Description: Reykjavik, Iceland

- Code: ITMI
- Description: Milan, Italy

- Code: ITRO
- Description: Rome, Italy

- Code: ITTU
- Description: Turin, Italy

- Code: JESH
- Description: St. Helier, Channel Islands, Jersey

- Code: JMKI
- Description: Kingston, Jamaica

- Code: JOAM
- Description: Amman, Jordan

- Code: JPTO
- Description: Tokyo, Japan

- Code: KENA
- Description: Nairobi, Kenya

- Code: KHPP
- Description: Phnom Penh, Cambodia

- Code: KRSE
- Description: Seoul, Republic of Korea

- Code: KWKC
- Description: Kuwait City, Kuwait

- Code: KYGE
- Description: George Town, Cayman Islands

- Code: KZAL
- Description: Almaty, Kazakhstan

- Code: LAVI
- Description: Vientiane, Laos

- Code: LBBE
- Description: Beirut, Lebanon

- Code: LKCO
- Description: Colombo, Sri Lanka

- Code: LULU
- Description: Luxembourg, Luxembourg

- Code: LVRI
- Description: Riga, Latvia

- Code: MACA
- Description: Casablanca, Morocco

- Code: MARA
- Description: Rabat, Morocco

- Code: MCMO
- Description: Monaco, Monaco

- Code: MNUB
- Description: Ulan Bator, Mongolia

- Code: MOMA
- Description: Macau, Macao

- Code: MTVA
- Description: Valletta, Malta

- Code: MUPL
- Description: Port Louis, Mauritius

- Code: MVMA
- Description: Male, Maldives

- Code: MWLI
- Description: Lilongwe, Malawi

- Code: MXMC
- Description: Mexico City, Mexico

- Code: MYKL
- Description: Kuala Lumpur, Malaysia

- Code: MYLA
- Description: Labuan, Malaysia

- Code: MZMA
- Description: Maputo, Mozambique

- Code: NAWI
- Description: Windhoek, Namibia

- Code: NGAB
- Description: Abuja, Nigeria

- Code: NGLA
- Description: Lagos, Nigeria

- Code: NLAM
- Description: Amsterdam, Netherlands

- Code: NLRO
- Description: Rotterdam, Netherlands

- Code: NOOS
- Description: Oslo, Norway

- Code: NPKA
- Description: Kathmandu, Nepal

- Code: NYFD
- Description: New York Fed Business Day (as defined in 2006 ISDA Definitions
  Section 1.9, 2000 ISDA Definitions Section 1.9, and 2021 ISDA Definitions
  Section 2.1.7)

- Code: NYSE
- Description: New York Stock Exchange Business Day (as defined in 2006 ISDA
  Definitions Section 1.10, 2000 ISDA Definitions Section 1.10, and 2021 ISDA
  Definitions Section 2.1.8)

- Code: NZAU
- Description: Auckland, New Zealand

- Code: New
- Description: Zealand Business Day (proposed effective date: 2025-10-06)

- Code: NZBD
- Description: New Zealand Business Day (proposed effective date: 2025-10-06)

- Code: NZWE
- Description: Wellington, New Zealand

- Code: OMMU
- Description: Muscat, Oman

- Code: PAPC
- Description: Panama City, Panama

- Code: PELI
- Description: Lima, Peru

- Code: PHMA
- Description: Manila, Philippines

- Code: PHMK
- Description: Makati, Philippines

- Code: PKKA
- Description: Karachi, Pakistan

- Code: PLWA
- Description: Warsaw, Poland

- Code: PRSJ
- Description: San Juan, Puerto Rico

- Code: PTLI
- Description: Lisbon, Portugal

- Code: QADO
- Description: Doha, Qatar

- Code: ROBU
- Description: Bucharest, Romania

- Code: RSBE
- Description: Belgrade, Serbia

- Code: RUMO
- Description: Moscow, Russian Federation

- Code: SAAB
- Description: Abha, Saudi Arabia

- Code: SAJE
- Description: Jeddah, Saudi Arabia

- Code: SARI
- Description: Riyadh, Saudi Arabia

- Code: SEST
- Description: Stockholm, Sweden

- Code: SGSI
- Description: Singapore, Singapore

- Code: SILJ
- Description: Ljubljana, Slovenia

- Code: SKBR
- Description: Bratislava, Slovakia

- Code: SLFR
- Description: Freetown, Sierra Leone

- Code: SNDA
- Description: Dakar, Senegal

- Code: SVSS
- Description: San Salvador, El Salvador

- Code: THBA
- Description: Bangkok, Thailand

- Code: TNTU
- Description: Tunis, Tunisia

- Code: TRAN
- Description: Ankara, Turkey

- Code: TRIS
- Description: Istanbul, Turkey

- Code: TTPS
- Description: Port of Spain, Trinidad and Tobago

- Code: TWTA
- Description: Taipei, Taiwan

- Code: TZDA
- Description: Dar es Salaam, Tanzania

- Code: TZDO
- Description: Dodoma, Tanzania

- Code: UAKI
- Description: Kiev, Ukraine

- Code: UGKA
- Description: Kampala, Uganda

- Code: USBO
- Description: Boston, Massachusetts, United States

- Code: USCH
- Description: Chicago, United States

- Code: USCR
- Description: Charlotte, North Carolina, United States

- Code: USDC
- Description: Washington, District of Columbia, United States

- Code: USD
- Description: -ICESWAP Publication dates for ICE Swap rates based on USD-LIBOR
  rates

- Code: USD
- Description: -MUNI Publication dates for the USD-Municipal Swap Index

- Code: USDN
- Description: Denver, United States

- Code: USDT
- Description: Detroit, Michigan, United States

- Code: USGS
- Description: U.S. Government Securities Business Day (as defined in 2006 ISDA
  Definitions Section 1.11 and 2000 ISDA Definitions Section 1.11)

- Code: USHL
- Description: Honolulu, Hawaii, United States

- Code: USHO
- Description: Houston, United States

- Code: USLA
- Description: Los Angeles, United States

- Code: USMB
- Description: Mobile, Alabama, United States

- Code: USMN
- Description: Minneapolis, United States

- Code: USNY
- Description: New York, United States

- Code: USPO
- Description: Portland, Oregon, United States

- Code: USSA
- Description: Sacramento, California, United States

- Code: USSE
- Description: Seattle, United States

- Code: USSF
- Description: San Francisco, United States

- Code: USWT
- Description: Wichita, United States

- Code: UYMO
- Description: Montevideo, Uruguay

- Code: UZTA
- Description: Tashkent, Uzbekistan

- Code: VECA
- Description: Caracas, Venezuela

- Code: VGRT
- Description: Road Town, Virgin Islands (British)

- Code: VNHA
- Description: Hanoi, Vietnam

- Code: VNHC
- Description: Ho Chi Minh (formerly Saigon), Vietnam

- Code: YEAD
- Description: Aden, Yemen

- Code: ZAJO
- Description: Johannesburg, South Africa

- Code: ZMLU
- Description: Lusaka, Zambia

- Code: ZWHA
- Description: Harare, Zimbabwe


*** Business unit entity                                               :code:

Represents internal organizational units (e.g., desks, departments, branches).
Supports hierarchical structure.

| Field                   | Type | Description                                   | Foreign Key Reference              |
|-------------------------+------+-----------------------------------------------+------------------------------------|
| unit_id                 | INT  | Primary key                                   | —                                  |
| party_id                | UUID | Top-level legal entity this unit belongs to   | → party                            |
| parent_business_unit_id | INT  | References parent unit (self-referencing)     | → business_unit.unit_id (nullable) |
| unit_name               | TEXT | Human-readable name (e.g., "FX Options Desk") | —                                  |
| unit_id_code            | TEXT | Optional internal code or alias               | —                                  |
| business_centre         | TEXT | Business centre for the unit                  | → business centre scheme           |

business_centre may be null (for example, we may want to have global desk and
then London desk.

*** Book and Portfolio entities                                        :code:

Support a single, unified hierarchical tree for risk aggregation and reporting
(Portfolios) while maintaining operational accountability and legal/bookkeeping
boundaries at the leaf level (Books).

**** Portfolio

Logical Aggregation Nodes. Represents organizational, risk, or reporting
groupings. Never holds trades directly.

| Field               | Type      | Description                                                                           |
|---------------------+-----------+---------------------------------------------------------------------------------------|
| portfolio_id (PK)   | UUID      | Globally unique identifier.                                                           |
| parent_portfolio_id | UUID (FK) | Self-referencing FK. NULL = root node.                                                |
| name                | TEXT      | Human-readable name (e.g., "Global Rates", "APAC Credit").                            |
| owner_unit_id       | INT (FK)  | Business unit (desk/branch) responsible for management.                               |
| purpose_type        | ENUM      | 'Risk', 'Regulatory', 'ClientReporting', 'Internal'.                                  |
| aggregation_ccy     | CHAR(3)   | Currency for P&L/risk aggregation at this node (ISO 4217).                            |
| is_virtual          | BOOLEAN   | If true, node is purely for on-demand reporting (not persisted in trade attribution). |
| created_at          | TIMESTAMP | Audit trail.                                                                          |

Note: Portfolios do not have a legal_entity_id. Legal context is derived from
descendant Books.

**** Book

Operational Ledger Leaves. The only entity that holds trades. Serves as the
basis for accounting, ownership, and regulatory capital treatment.

| Field               | Type      | Description                                                                         |
|---------------------+-----------+-------------------------------------------------------------------------------------|
| book_id (PK)        | UUID      | Globally unique identifier.                                                         |
| parent_portfolio_id | UUID (FK) | Mandatory: Links to exactly one portfolio.                                          |
| name                | TEXT      | Must be unique within legal entity (e.g., "FXO_EUR_VOL_01").                        |
| legal_entity_id     | UUID (FK) | Mandatory: References party.party_id (must be an LEI-mapped legal entity).          |
| ledger_ccy          | CHAR(3)   | Functional/accounting currency (ISO 4217).                                          |
| gl_account_ref      | TEXT      | Reference to external GL (e.g., "GL-10150-FXO"). May be nullable if not integrated. |
| cost_center         | TEXT      | Internal finance code for P&L attribution.                                          |
| book_status         | ENUM      | 'Active', 'Closed', 'Frozen'.                                                       |
| is_trading_book     | BOOLEAN   | Critical for Basel III/IV: distinguishes Trading vs. Banking Book.                  |
| created_at          | TIMESTAMP | For audit.                                                                          |
| closed_at           | TIMESTAMP | When book_status = 'Closed'.                                                        |

Objectives:

- Strict separation: Portfolios = logical; Books = operational
- Legal ownership at Book level → critical for regulatory capital, legal netting, tax
- Hierarchy via parent_portfolio_id
- Trading vs. Banking book flag → Basel requirement

Hierarchy Integrity Constraints:

- Rule: A Portfolio must not directly contain another Portfolio and a Book at
  the same level if that violates business policy.
  - Enforce via application logic or DB constraint (e.g., CHECK that a Portfolio
    is either "container-only" or "leaf-container", but typically Portfolios can
    contain both sub-Portfolios and Books—this is normal).
- Cycle Prevention: Ensure no circular references (parent → child → parent). Use
  triggers or application validation.

- Multi-Legal Entity Support: Your model allows Books under the same Portfolio
  to belong to different legal entities. Is this intentional?
  - Allowed in some firms (for consolidated risk views).
  - Forbidden in others (e.g., regulatory ring-fencing).
  - Recommendation: Add a validation rule (application-level): If a Portfolio
    contains any Books, all descendant Books must belong to the same
    legal_entity_id.” Or, if mixed entities are allowed, flag the Portfolio as
    'MultiEntity' in purpose_type.

- Trade Ownership: Explicitly state: Every trade must have a book_id (FK). No
  trade exists outside a Book. This is implied but should be documented as a
  core invariant.

- Lifecycle & Governance: Add version or valid_from/valid_to if Books/Portfolios
  evolve over time (e.g., name changes, reorgs).
  - Especially important for audit and historical P&L.

- Consider owner_person_id (trader or book manager) for Books.

- Naming & Uniqueness:
  Enforce: (legal_entity_id, name) must be unique for Books.
  - Prevents ambiguous book names like "RatesDesk" used by two entities.

- Book Closure Policy: When a Book is Closed, should existing trades remain?
  - Yes (typical). But no new trades allowed.
  - Your book_status covers this

Combined Hierarchy Rules (Refined):

| Rule                | Description                                                                                                   |
|---------------------+---------------------------------------------------------------------------------------------------------------|
| Leaf Invariant      | Only Books may hold trades. Portfolios are purely aggregators.                                                |
| Single Parent       | Every Book and non-root Portfolio has exactly one parent.                                                     |
| Legal Entity Scope  | A Book declares its legal owner. A Portfolio’s legal scope is the union of its Books’ entities.               |
| Permissioning       | Trade permission → granted on book_id. View/Analyze permission → granted on portfolio_id (includes subtree)   |
| Accounting Boundary | P&L, capital, and ledger entries are computed per Book, then rolled up through Portfolios in aggregation_ccy. |

*** Accounts need to have a human or robot field                       :code:

Look for correct terminology (actor type?).

*** Geo-location tests fail for some IP Addresses                      :code:

Error is probably happening because the range is not supposed to be used.

#+begin_src xml
<Catch2TestRun name="ores.geo.tests" rng-seed="3117545047" xml-format-version="3" catch2-version="3.12.0">
  <TestCase name="geolocation_result_default_construction" tags="[service][#geolocation_service_tests]" filename="/home/marco/Development/OreStudio/OreStudio.local1/projects/ores.geo/tests/geolocation_service_tests.cpp" line="38">
    <OverallResult success="true" skips="0" durationInSeconds="1.00031"/>
  </TestCase>
  <TestCase name="lookup_with_boost_asio_address" tags="[service][#geolocation_service_tests]" filename="/home/marco/Development/OreStudio/OreStudio.local1/projects/ores.geo/tests/geolocation_service_tests.cpp" line="90">
    <Expression success="false" type="CHECK" filename="/home/marco/Development/OreStudio/OreStudio.local1/projects/ores.geo/tests/geolocation_service_tests.cpp" line="102">
      <Original>
        result.error() == geolocation_error::address_not_found
      </Original>
      <Expanded>
        2 == 1
      </Expanded>
    </Expression>
    <OverallResult success="false" skips="0" durationInSeconds="1.00013"/>
  </TestCase>
#+end_src

We don't see anything in the logs:

#+begin_src logview
2026-01-09 19:16:19.370327 [INFO] [ores.testing.test_database_manager] Database context created successfully
2026-01-09 19:16:19.370360 [DEBUG] [ores.geo.service.geolocation_service] Geolocation lookup for 10.0.0.1. SQL: SELECT country_code FROM ores.geoip_lookup('10.0.0.1'::inet)
2026-01-09 19:16:19.386331 [DEBUG] [ores.geo.service.geolocation_service] Geolocation lookup for 10.0.0.1. Total rows: 0
2026-01-09 19:16:19.386475 [INFO] [ores.geo.tests] Lookup result for boost::asio::ip::address
2026-01-09 19:16:19.386695 [DEBUG] [catch2] Section ended: lookup_with_boost_asio_address (assertions: 1)
2026-01-09 19:16:19.386741 [INFO] [catch2] Test case ended: lookup_with_boost_asio_address - PASSED
2026-01-09 19:16:19.386767 [INFO] [catch2]   Assertions: 1 passed, 0 failed, 1 total
#+end_src

We should write the assertions in the logs.


*** Contact information entity                                         :code:

Contact Information is a container that groups various ways to reach an entity.

Contact Information can be associated with either a Party (at the legal entity
level) or a BusinessUnit (at the desk/operational level). To build a robust
trading system, your database should support a polymorphic or flexible link to
handle this.

The Logic of the Link:

- Link to Party: Used for Legal and Regulatory contact details. This is the
  "Head Office" address, the legal service of process address, or the general
  firm-wide contact for the LEI.
- Link to Business Unit: Used for Execution and Operational contact details.
  This is where your "Machine" or "Human" actually lives. It links the trader or
  algo to a specific desk's phone number, email, and—most importantly—its
  Business Center (Holiday Calendar).

**** Type: Contact Information

This is the main container for how to reach a party or person.

- address (Complex): The physical location.
- phone (String): Multiple entries allowed (Work, Mobile, Fax).
- email (String): Electronic mail addresses.
- webPage (String): The entity's URL.

**** Type: Address

The physical street address structure.

- streetAddress (Complex): Usually a list of strings (Line 1, Line 2, etc.).
- city (String): The city or municipality.
- state (String): The state, province, or region.
- country (Scheme): An ISO 3166 2-letter country code (e.g., US, GB).
- postalCode (String): The ZIP or Postcode.

*** Add support for staging                                            :code:

- on import, are we importing one currency at a time? should import the entire
  file.

*** Non-classified observations                                    :analysis:

Observations, snags and so forth that have not been analysed to form a proper
story.

- add configuration option not to confirm on quit.
- no repository tests for change reason. We need to check all entities and the
  coverage.
- qt code processing for network error messages is duplicated.
- raw SQL in image repository, should be a function.
- session should record if telemetry is enabled or not enabled.
- bootstrap mode flag change does not generate new version.
- it is possible to create a country without a name. likely a currency too.
- should be possible to click on user and see user profile details. If the user
  is admin, show all details; if not, show only key ones like email, etc.
- remove maximise button.
- test suites need to log version at the start. info script needs to grep log
  for version for all suites.
- no generators for roles and permissions it seems.
- we still have the legacy password salt field.
- should role permission tables have change reasons?
- missing ores.analyser from system model.
- add flag "is iso compliant" or some such for currencies and countries which
  are in the standard.
- allow users displaying password in password boxes.
- send and receive are empty in sessions.
- next, can you explain the difference in window types between the main currency
  dialog and say currency history and currency details. we switched of minimise
  button on the currency history and currency details and that worked fine. we
  tried switching it off on the main currency window twice and that never seems
  to work. can we make all windows the same type? same as currency history /
  currency details.
- merge common functionality between entities in ores.qt.
- updating email from my account does not raise an event.
- fictional countries should also have X prefixes
- generate button should only exist in new, not edit.
- no save button in currency list, need for generation
- would be nice to be able to "locally modify" entity and then press save for
  the batch. in git we call this staging, what name should we use? and what
  icon?
- server should peridically house keep sessions. if not connected, just mark it
  as orphan. also when connection is dropped mark session as finished.
- shouldn't revert to version be a server side operation? e.g. current version,
  target version.
- in the history diff, add a "from version" combo which shows either latest or
  previous.

*** Add a flag for human readable time                                 :code:

We should be able to use regular timestamps or human readable time.

*** Retrieve NAT'ed IP address from local IP                           :code:

It would be nice to be able to resolve to the NAT'ed IP address. Gemini:

#+begin_src cpp
#include <iostream>
#include <string>
#include <boost/asio.hpp>

using boost::asio::ip::tcp;

std::string get_public_ip() { try { boost::asio::io_context io_context;

    // 1. Resolve the address for api.ipify.org
    tcp::resolver resolver(io_context);
    tcp::resolver::results_type endpoints = resolver.resolve("api.ipify.org", "http");

    // 2. Connect to the server
    tcp::socket socket(io_context);
    boost::asio::connect(socket, endpoints);

    // 3. Formulate the HTTP GET request
    std::string request =
        "GET / HTTP/1.1\r\n"
        "Host: api.ipify.org\r\n"
        "Connection: close\r\n\r\n";

    // 4. Send the request
    boost::asio::write(socket, boost::asio::buffer(request));

    // 5. Read the response
    boost::asio::streambuf response;
    boost::asio::read_until(socket, response, "\r\n");

    // Check the status line (optional but recommended)
    std::istream response_stream(&response);
    std::string http_version;
    unsigned int status_code;
    response_stream >> http_version >> status_code;

    if (status_code != 200) {
        return "Error: HTTP Status " + std::to_string(status_code);
    }

    // Skip the HTTP headers
    boost::asio::read_until(socket, response, "\r\n\r\n");

    // The remaining data in the buffer (and what's left to read) is the IP
    std::string public_ip;
    while (boost::asio::read(socket, response, boost::asio::transfer_at_least(1), boost::system::error_code())) {
        // Keep reading until EOF
    }

    // Convert the body content to a string
    std::stringstream ss;
    ss << &response;
    public_ip = ss.str();

    // api.ipify.org returns only the IP as plain text in the body
    return public_ip;

} catch (std::exception& e) {
    return std::string("Exception: ") + e.what();
}
}

int main() {
        stdout::cout << "Fetching public IP..." << std::endl;
        std::string ip = get_public_ip();
        std::cout << "Your Public IP is: " << ip << std::endl;
        return 0;
}
#+end_src


*** Users should be able to add picture to profile                     :code:

It would be useful to have avatars. We can then display those in other places.

*** Add support for protocol debugging in Qt                           :code:

We should add a view of all messages sent and received like the comms champ
tools. It should be possible to record a session to file (maybe all the
requests?) and then replay that session. You should be able to do it from the
UI.

Links:

- [[https://github.com/commschamp/cc_tools_qt][GH: cc_tools_qt]]: "This project contains tool application(s), which can be used
  to develop, monitor and debug custom binary communication protocols, that were
  developed using the COMMS Library. All the applications are plug-in based,
  i.e. plug-ins are used to define I/O socket, data filters, and the custom
  protocol itself. The tools use Qt framework for GUI interfaces as well as
  loading and managing plug-ins."
- [[https://github.com/commschamp/cc_tools_qt/wiki/How-to-Use-CommsChampion-Tools][How to Use CommsChampion Tools]]: screenshots of the UI to explore the protocol.

*** Assorted UI polish work                                            :code:

Break these down into their own stories:

- icons for CRUD are not enabling when on detached mode.
- no status bar in dettached mode.
- should we just have toolbars at the detached window level?
- application should exit when main window is closed.
- is it possible to dock windows like visual studio?
- add a detach current window that just detaches that window.
- disabled menu options are not properly greyed out. Done.

*** Instrument components with telemetry context                       :code:

Now that logging has been integrated with telemetry, the next step is to
instrument key components with the =TLOG_SEV= macro to enable trace correlation.

Tasks:

- Instrument =server_session= with root span on connection, child spans per
  request.
- Instrument =client_session= with spans for outgoing requests.
- Pass =telemetry_context= through message handlers in ores.iam, ores.refdata, etc.
- Add spans for database operations in ores.database.

*** Add a data library or collection                                   :code:

Datasets are logically related. We should have collections of datasets designed
to work together.

*** Add widget to manage assets                                        :code:

We can't upload flags etc.

Merged stories:

*Add an image browser*

At present we can't add or remove images.

- update existing image browser to show tags and allow switching them on and
  off.

*** Missing session properties                                         :code:

- disconnect type: orphaned, client disconnect.
- add version, commit, etc.

*** Update currency details to use tabs                                :code:

We need a main tab with the currency related properties, then a "system" tab
with temporal data which is read-only even on edit and a "image" tab with the
image used to represent the currency. It could also contain some description or
notes.


*** Locking an account should log user out                             :code:

At present if you lock an account the user will remain logged in.

*** Review of Wt                                                       :code:

Problems:

- no icons in website for the tab.
- no flags in currencies in wt.
- no way of knowing about reload (eventing) in wt. we should make the reload
  button change colour?
- currency edit window in wt is too large, can't see bottom of screen.
- cannot edit account in wt.
- iso code field too small, numeric code field too small
- adding new account crashes wt. saving new currency crashes wt.

#+begin_src
[2026-Jan-07 12:16:08.080] 1050920 - [access] "wthttp: 127.0.0.1   POST /?wtd=kh0IzYWIDev5vqgQ HTTP/1.1 200 271"
[2026-Jan-07 12:16:08.080] 1050920 - [info] "WebRequest: took 0.969 ms"
#+end_src

#+begin_src logviewer
2026-01-07 12:16:15.578713 [TRACE] [ores.iam.repository.account_mapper] Mapping db entity: {"id":"019b9856-215c-7fd4-b139-11821e39d80a","version":1,"username":"newuser3","password_hash":"$scrypt$ln=14,r=8,p=1$wcZS3a7UEPEz8RBhIuOWIA==$vFYxagrQLQ1FXzMhP1u1D4xMJF2G4HMMsmoOxpRsdwYjSChBIsGiuA92cD8dPODlSYoG9uiX6SohLcUaZNUUUg==","password_salt":"","totp_secret":"","email":"newuser3@example.com","modified_by":"bootstrap","valid_from":"2026-01-07 12:02:20","valid_to":"9999-12-31 23:59:59"}
2026-01-07 12:16:15.578787 [TRACE] [ores.iam.repository.account_mapper] Mapped db entity. Result: {"version":1,"id":"019b9856-215c-7fd4-b139-11821e39d80a","recorded_by":"bootstrap","username":"newuser3","password_hash":"$scrypt$ln=14,r=8,p=1$wcZS3a7UEPEz8RBhIuOWIA==$vFYxagrQLQ1FXzMhP1u1D4xMJF2G4HMMsmoOxpRsdwYjSChBIsGiuA92cD8dPODlSYoG9uiX6SohLcUaZNUUUg==","password_salt":"","totp_secret":"","email":"newuser3@example.com","recorded_at":"2026-01-07 12:02:20.000000000Z"}
2026-01-07 12:16:15.578866 [DEBUG] [ores.iam.repository.account_mapper] Mapped db entities.
2026-01-07 12:16:15.578895 [DEBUG] [ores.iam.repository.login_info_repository] Reading all login_info.
2026-01-07 12:16:15.579395 [DEBUG] [ores.iam.repository.login_info_repository] Read all login_info. Total: 1
2026-01-07 12:16:15.579436 [DEBUG] [ores.iam.repository.login_info_mapper] Mapping db entities. Total: 1
2026-01-07 12:16:15.579465 [TRACE] [ores.iam.repository.login_info_mapper] Mapping db entity: {"account_id":"019b9856-215c-7fd4-b139-11821e39d80a","last_ip":"127.0.0.1","last_attempt_ip":"127.0.0.1","failed_logins":0,"locked":0,"last_login":"2026-01-07 12:14:01","online":1,"password_reset_required":0}
2026-01-07 12:16:15.579519 [TRACE] [ores.iam.repository.login_info_mapper] Mapped db entity. Result: {"last_login":"2026-01-07 12:14:01.000000000Z","account_id":"019b9856-215c-7fd4-b139-11821e39d80a","failed_logins":0,"locked":false,"online":true,"password_reset_required":false,"last_ip":"127.0.0.1","last_attempt_ip":"127.0.0.1"}
2026-01-07 12:16:15.579585 [DEBUG] [ores.iam.repository.login_info_mapper] Mapped db entities.
#+end_src

*** Add a dashboard for users                                          :code:

As per screenshots:

[[./user_dashboard_core_ui.png]]

[[./dashboard_ideas.png]]

[[./dashboard_ideas_II.png]]

Notes:

- add command to shell to list sessions.

*** Add sign-up approval workflow                                      :code:

Should also handle invite codes, etc.

We did most of the work except invite codes.

*** Listen for events in details dialog                                :code:

At present we are only listening for details in the main dialogs (accounts,
currencies). It would be nice to be able to listen to events in the details
dialogs. however, it needs to listen to events only for that specific entity
(e.g. currency pair, account etc). Not sure we are setup for this.

* Footer

|                                                         |
|---------------------------------------------------------|
| Previous: [[../../agile/v0/version_zero.org][Version Zero]] |
