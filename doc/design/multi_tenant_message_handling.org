* Multi-Tenant Message Handling Requirements

** Overview

This document specifies requirements for properly supporting multi-tenant
message handling in ORE Studio's communication layer. The goal is to ensure
correct tenant isolation while maintaining high performance under load.

** Functional Requirements

*** FR-1: Session-Tenant Binding

Each authenticated session MUST be bound to exactly one tenant for its entire
lifetime.

- When a user authenticates, the session is bound to the tenant of their account
- The tenant binding is immutable for the session duration
- Session metadata MUST include the tenant_id for efficient access
- Unauthenticated sessions have no tenant context and can only perform
  authentication operations

*** FR-2: Tenant-Scoped Message Processing

Every message received from an authenticated session MUST be processed in the
context of that session's bound tenant.

- All database operations MUST use the correct tenant_id
- Row-Level Security (RLS) policies MUST be activated with the correct tenant
- No message handler may assume or hardcode a tenant context
- The tenant context MUST be established before any business logic executes

*** FR-3: Tenant Isolation Guarantee

Messages from Tenant A MUST NEVER execute in Tenant B's context.

- Tenant context MUST NOT be shared across concurrent requests
- Mutable tenant state MUST NOT exist in shared objects (handlers, services)
- Each request MUST have isolated tenant context
- Cross-tenant data access is prohibited except through explicit system
  operations

*** FR-4: System Tenant Operations

Certain privileged operations require cross-tenant or system-level access.

- SuperAdmin accounts (system tenant) MAY access data across all tenants
- System maintenance operations MAY bypass tenant restrictions
- Audit logging of cross-tenant operations is mandatory
- Bootstrap mode operates in system tenant context

*** FR-5: Tenant Lifecycle Handling

The system MUST handle tenant state changes gracefully.

- If a tenant is suspended, active sessions SHOULD be terminated
- If a tenant is deleted, all sessions MUST be invalidated
- Tenant lookup failures MUST result in request rejection, not fallback

** Non-Functional Requirements

*** NFR-1: Performance - Request Latency

Tenant context establishment MUST NOT significantly impact request latency.

- Target: < 1ms overhead per request for tenant context setup
- Tenant_id SHOULD be cached in session state (already resolved at login)
- No database lookup required per-request for tenant resolution
- Connection pool access SHOULD be O(1) with respect to tenant

*** NFR-2: Performance - Throughput

The system MUST support high concurrent request rates.

- Target: 10,000+ requests/second across all tenants
- No global locks for tenant context management
- Tenant context MUST be thread-local or request-local
- Message handlers MUST be stateless with respect to tenant

*** NFR-3: Performance - Connection Pooling

Database connections MUST be efficiently managed across tenants.

- Single connection pool shared across all tenants (current design)
- Tenant context set via session variable (app.current_tenant_id)
- Connection acquisition MUST NOT require tenant-specific pools
- RLS handles data isolation at the query level

*** NFR-4: Memory Efficiency

Tenant context management MUST have bounded memory overhead.

- No per-tenant service/repository instances cached long-term
- Temporary objects created per-request are acceptable (stack allocation)
- Session state should be minimal (tenant_id, account_id, permissions)

*** NFR-5: Correctness Under Concurrency

The system MUST maintain correctness under concurrent multi-tenant load.

- No race conditions in tenant context setting
- Thread-safe session state access
- Atomic tenant context establishment per request

** Constraints

*** C-1: Existing Architecture

The solution must work within the existing architecture constraints:

- Message handlers are singleton/long-lived objects
- Services and repositories are created with a database::context
- database::context is a value type that owns connection pool reference
- RLS is already implemented and functional
- Session state is managed per-connection

*** C-2: Backward Compatibility

The refactoring should minimize breaking changes:

- External API (message types) should remain unchanged
- Client code should not require modification
- Database schema changes should be avoided if possible

** Current Architecture Analysis

*** Session State

Currently, sessions store:
- account_id (UUID)
- principal (contains hostname, used for tenant lookup)

Missing:
- tenant_id (requires lookup on each request currently)

*** Message Handler Pattern

Current pattern (problematic):
#+begin_src cpp
class some_message_handler {
    database::context ctx_;           // Shared, mutable tenant state!
    some_service service_{ctx_};      // Service caches context copy

    response handle(request req, principal p) {
        // Problem: ctx_ tenant may be wrong for this request
        // Problem: service_ has stale tenant context
    }
};
#+end_src

*** Database Context

The database::context class:
- Holds reference to tenant_aware_pool
- tenant_aware_pool has mutable tenant_id_ member
- When context is copied, copies share the pool reference
- Setting tenant_id on one copy affects all copies (race condition!)

** Detailed Architectural Analysis

*** Message Handlers Inventory

The following message handlers exist and share the same architectural pattern:

| Handler                    | Component       | Services Cached                           |
|----------------------------+-----------------+-------------------------------------------|
| accounts_message_handler   | ores.iam        | account_service, setup_service, repos     |
| risk_message_handler       | ores.refdata    | currency_service, country_service         |
| dq_message_handler         | ores.dq         | 8 services (change mgmt, datasets, etc.)  |
| assets_message_handler     | ores.assets     | image_repository                          |
| telemetry_message_handler  | ores.telemetry  | (to be analyzed)                          |
| variability_message_handler| ores.variability| (to be analyzed)                          |

*** Current Handler Pattern (Problematic)

All handlers follow this pattern:

#+begin_src cpp
class some_message_handler {
private:
    database::context ctx_;              // (1) Handler owns context
    some_service service_{ctx_};         // (2) Service copies context internally
    another_service other_{ctx_};        // (3) Multiple services, each with copy

public:
    handler_result handle_message(..., const std::string& remote_address) {
        // (4) No tenant_id available from interface
        // (5) Must look up session to get tenant - expensive
        // (6) ctx_.set_tenant_id() affects shared state - RACE CONDITION
        auto session = sessions_->get_session(remote_address);
        // What tenant is this session? We don't know!
    }
};
#+end_src

*** Session State Gap

Current ~session_data~ structure:

#+begin_src cpp
struct session_data {
    boost::uuids::uuid id;
    boost::uuids::uuid account_id;
    std::string username;
    // ... other fields ...
    // MISSING: tenant_id!
};
#+end_src

The session does NOT store the tenant_id, requiring a database lookup each time.

*** database::context Analysis

#+begin_src cpp
class context {
    tenant_aware_pool<connection_type> connection_pool_;  // Owns pool wrapper
    const sqlgen::postgres::Credentials credentials_;

    void set_tenant_id(std::string tenant_id) {
        connection_pool_.set_tenant_id(std::move(tenant_id));  // Mutates pool
    }
};
#+end_src

Critical issue: ~tenant_aware_pool~ stores ~tenant_id_~ as a mutable member:

#+begin_src cpp
template <class Connection>
class tenant_aware_pool {
    sqlgen::ConnectionPool<Connection> pool_;  // Underlying pool
    std::string tenant_id_;                    // MUTABLE STATE

    void set_tenant_id(std::string tenant_id) {
        tenant_id_ = std::move(tenant_id);     // NOT THREAD-SAFE
    }
};
#+end_src

When multiple requests execute concurrently:
1. Request A (tenant X) sets tenant_id_ to X
2. Request B (tenant Y) sets tenant_id_ to Y
3. Request A acquires connection - gets Y's context! DATA LEAK!

*** Impact Analysis

**** Affected Operations

Every operation that uses:
- Account service (create, update, list, authenticate)
- Currency/Country services
- DQ services (change reasons, datasets, catalogs, etc.)
- Asset repository
- Session repository
- Any service using database::context

**** Security Impact

- Cross-tenant data access (read data from wrong tenant)
- Cross-tenant data modification (write to wrong tenant)
- Authentication bypass (session created in wrong tenant)
- Audit trail corruption (operations logged to wrong tenant)

** Proposed Solution: Shared Pool with Per-Request Tenant Wrappers

*** Design Principle

Tenant context must flow from authenticated session through the entire request:

#+begin_example
Session (has tenant_id) → Request Context → Service → Repository → Database
                               ↓
                     [immutable for request lifetime]
#+end_example

*** Connection Pool Architecture

The key insight is that ~sqlgen::ConnectionPool~ uses ~rfl::Ref~ (which wraps
~std::shared_ptr~) for its internal connection storage. This means copying a
~ConnectionPool~ shares the underlying connections rather than duplicating them.

#+begin_example
                        ┌─────────────────────────────────────┐
                        │   sqlgen::ConnectionPool            │
                        │   (shared via rfl::Ref, ~10 conns)  │
                        └─────────────────┬───────────────────┘
                                          │ shared_ptr semantics
            ┌─────────────────────────────┼─────────────────────────────┐
            │                             │                             │
            ▼                             ▼                             ▼
  ┌─────────────────────┐   ┌─────────────────────┐   ┌─────────────────────┐
  │ tenant_aware_pool   │   │ tenant_aware_pool   │   │ tenant_aware_pool   │
  │ tenant_id = "AAA"   │   │ tenant_id = "BBB"   │   │ tenant_id = "CCC"   │
  │ (request 1)         │   │ (request 2)         │   │ (request 3)         │
  └─────────────────────┘   └─────────────────────┘   └─────────────────────┘
            │                             │                             │
            ▼                             ▼                             ▼
  SET app.current_tenant   SET app.current_tenant   SET app.current_tenant
       = 'AAA'                  = 'BBB'                  = 'CCC'
#+end_example

Each request creates a lightweight ~tenant_aware_pool~ wrapper (~40 bytes: string
+ shared_ptr copy) that shares the same underlying database connections.

*** What Happens on Connection Acquire

1. Request comes in from authenticated session (tenant_id known)
2. Create ~tenant_aware_pool~ wrapper with session's tenant_id
3. When ~acquire()~ is called:
   - Get connection from shared pool (same ~10 connections for everyone)
   - Execute ~SET app.current_tenant_id = 'this-wrapper's-tenant'~
   - Return connection for use
4. Query executes with RLS using that tenant_id
5. Connection returns to shared pool

The ~SET app.current_tenant_id~ is a PostgreSQL session variable set fresh on
every acquire. This already happens in the current code - no additional database
overhead.

*** Architecture Comparison

**** Current (Problematic)

#+begin_src
┌────────────────────────────────────────────────────────┐
│ Message Handler (singleton)                             │
│   └── database::context ctx_                           │
│         └── tenant_aware_pool                          │
│               ├── sqlgen::ConnectionPool (shared_ptr)  │
│               └── tenant_id_ ← MUTABLE, RACE CONDITION │
│                                                         │
│   └── account_service service_{ctx_}  ← stale tenant   │
│   └── other_service other_{ctx_}      ← stale tenant   │
└────────────────────────────────────────────────────────┘
#+end_src

**** Proposed (Safe)

#+begin_src
┌────────────────────────────────────────────────────────┐
│ Message Handler (singleton)                             │
│   └── sqlgen::ConnectionPool pool_  (shared, immutable)│
│   └── credentials_                  (immutable)        │
│                                                         │
│ Per-Request (on stack, discarded after):               │
│   └── tenant_aware_pool(pool_, session.tenant_id)      │
│         └── tenant_id_ ← IMMUTABLE for this request    │
│   └── database::context(pool_wrapper, credentials_)    │
│   └── account_service(request_context)                 │
└────────────────────────────────────────────────────────┘
#+end_src

*** Solution Components

**** 1. Add tenant_id to session_data

#+begin_src cpp
struct session_data {
    boost::uuids::uuid id;
    boost::uuids::uuid account_id;
    boost::uuids::uuid tenant_id;  // ADD THIS
    std::string username;
    // ... rest unchanged
};
#+end_src

Set at login time, immutable for session lifetime.

**** 2. Store raw pool in handlers (no tenant)

#+begin_src cpp
class some_message_handler {
private:
    sqlgen::ConnectionPool<Connection> pool_;  // Shared, no tenant
    sqlgen::postgres::Credentials credentials_;
    // NO cached services - create per-request
};
#+end_src

**** 3. Create per-request context with tenant

#+begin_src cpp
handler_result handle_get_accounts(..., const std::string& remote_address) {
    auto session = sessions_->get_session_data(remote_address);
    if (!session) {
        return error(authentication_failed);
    }

    // Create lightweight wrapper with correct tenant (shares pool)
    tenant_aware_pool scoped_pool(pool_, to_string(session->tenant_id));
    database::context request_ctx(std::move(scoped_pool), credentials_);

    // Create temporary services for this request
    account_service svc(request_ctx);
    auto accounts = svc.list_accounts();

    return serialize(accounts);
}
#+end_src

**** 4. Make tenant_aware_pool immutable

Remove ~set_tenant_id()~ method entirely:

#+begin_src cpp
template <class Connection>
class tenant_aware_pool {
    sqlgen::ConnectionPool<Connection> pool_;
    const std::string tenant_id_;  // IMMUTABLE - set in constructor only

    // NO set_tenant_id() method - tenant is fixed at construction
};
#+end_src

*** Performance Analysis

**** No New Connections

All ~tenant_aware_pool~ wrappers share the same underlying ~10 connections.
Creating a wrapper just copies a ~shared_ptr~ and a ~string~ - no database
operations.

**** Per-Request Overhead

| Operation                        | Cost           |
|----------------------------------+----------------|
| Copy shared_ptr (pool reference) | ~10 ns         |
| Copy tenant_id string (~36 chars)| ~50 ns         |
| Create context wrapper           | ~20 ns         |
| Create service (stores context)  | ~30 ns         |
| SET app.current_tenant_id        | Already exists |
|----------------------------------+----------------|
| Total additional overhead        | < 1 μs         |

**** Memory Per Request

| Component               | Size     |
|-------------------------+----------|
| tenant_aware_pool       | ~40 bytes|
| database::context       | ~48 bytes|
| service (stores context)| ~64 bytes|
|-------------------------+----------|
| Total (stack allocated) | < 200 bytes|

All stack-allocated, freed when request completes.

**** Avoiding Database Lookups

With tenant_id cached in session_data:
- Login: Lookup tenant once, store in session
- Subsequent requests: Read from in-memory session map (O(1))
- No database query per request for tenant resolution

** Implementation Plan

*** Phase 1: Add tenant_id to Sessions

Files to modify:
- ~ores.comms/include/ores.comms/service/session_data.hpp~

Changes:
1. Add ~boost::uuids::uuid tenant_id~ field to ~session_data~ struct
2. Add ~tenant_id~ to ~session_info~ struct for backward compatibility

*** Phase 2: Store tenant_id at Login

Files to modify:
- ~ores.iam/src/messaging/accounts_message_handler.cpp~ (handle_login_request)
- ~ores.iam/src/repository/session_repository.cpp~ (if session is persisted)

Changes:
1. During login, resolve tenant_id from account
2. Store tenant_id in session_data when creating session
3. Persist tenant_id if sessions are stored in database

*** Phase 3: Make tenant_aware_pool Immutable

Files to modify:
- ~ores.database/include/ores.database/domain/tenant_aware_pool.hpp~
- ~ores.database/include/ores.database/domain/context.hpp~

Changes:
1. Remove ~set_tenant_id()~ method from ~tenant_aware_pool~
2. Make ~tenant_id_~ member const
3. Remove ~set_tenant_id()~ method from ~context~ class
4. Ensure ~ConnectionPool~ copy shares underlying connections (verified)

*** Phase 4: Refactor Message Handlers

For each of the 6 handlers:

| Handler                    | Component        |
|----------------------------+------------------|
| accounts_message_handler   | ores.iam         |
| risk_message_handler       | ores.refdata     |
| dq_message_handler         | ores.dq          |
| assets_message_handler     | ores.assets      |
| telemetry_message_handler  | ores.telemetry   |
| variability_message_handler| ores.variability |

Changes per handler:
1. Replace ~database::context ctx_~ with ~sqlgen::ConnectionPool pool_~
2. Store ~credentials_~ separately
3. Remove cached service member variables
4. Add helper: ~database::context make_request_context(session_data&)~
5. In each handler method, create services on the stack with request context

Example refactoring pattern:
#+begin_src cpp
// Before
class handler {
    database::context ctx_;
    account_service service_{ctx_};  // REMOVE
};

// After
class handler {
    sqlgen::ConnectionPool<Connection> pool_;
    sqlgen::postgres::Credentials credentials_;

    database::context make_request_context(const session_data& session) {
        tenant_aware_pool scoped(pool_, to_string(session.tenant_id));
        return database::context(std::move(scoped), credentials_);
    }
};
#+end_src

*** Phase 5: Testing and Verification

1. Unit tests: Verify ~tenant_aware_pool~ is immutable (no ~set_tenant_id~)
2. Integration tests: Multi-tenant concurrent request test
   - 10 connections across 5 tenants
   - 100 requests each
   - Verify tenant isolation
3. ThreadSanitizer build: Run under load to detect races
4. Performance benchmark: Measure latency before/after

** Verification Criteria

*** VC-1: Correctness Test

Create integration test with:
- 10 concurrent connections, 5 different tenants
- Each connection sends 100 requests
- Verify all operations executed in correct tenant context
- Verify no cross-tenant data leakage

*** VC-2: Performance Benchmark

Measure before/after:
- Request latency (p50, p95, p99)
- Throughput (requests/second)
- Memory usage under load
- Connection pool utilization

*** VC-3: Race Condition Detection

Run ThreadSanitizer-enabled build under multi-tenant load to detect any
remaining data races.

** Appendix: Alternative Approaches Considered

*** Option 1: Thread-Local Tenant Context

Store tenant_id in thread-local storage, set at request start.

Rejected because:
- Async coroutines can resume on different threads
- Complex to manage with Boost.Asio

*** Option 2: Tenant-Specific Connection Pools

Create separate connection pool per tenant.

Rejected because:
- Memory overhead (N pools × M connections)
- Connection limits per database
- Complex pool management

*** Option 3: Mutex-Protected Tenant State

Add mutex to tenant_aware_pool for thread-safe tenant_id mutation.

Rejected because:
- Serializes all database operations
- Severe performance impact
- Doesn't solve the fundamental design issue

*** Option 4: Pass tenant_id Explicitly (Chosen)

Pass tenant_id through the call chain from session to database.

Advantages:
- Clear ownership and data flow
- No hidden mutable state
- Thread-safe by construction
- Minimal performance overhead

** Summary

The current architecture has a fundamental flaw: mutable tenant state in shared
objects leads to race conditions under concurrent multi-tenant load.

*** Root Cause

~tenant_aware_pool~ has a mutable ~tenant_id_~ field that can be modified by any
thread via ~set_tenant_id()~. When multiple requests execute concurrently,
they race to set this field, causing requests to execute in the wrong tenant
context.

*** Solution

Share the underlying connection pool (which uses ~shared_ptr~ semantics via
~rfl::Ref~) but create lightweight, immutable ~tenant_aware_pool~ wrappers
per-request. Each wrapper has a fixed tenant_id set at construction.

*** Key Changes

1. *Add tenant_id to session_data* - resolved at login, immutable for session
2. *Remove set_tenant_id()* - make tenant_aware_pool immutable
3. *Store raw pool in handlers* - no tenant context at handler level
4. *Create per-request contexts* - each request gets its own tenant wrapper
5. *Create services per-request* - no cached services with stale context

*** Performance Impact

- No new database connections (pool is shared)
- Per-request overhead: < 1 μs (wrapper creation)
- Memory overhead: < 200 bytes per request (stack allocated)
- Tenant lookup: O(1) from in-memory session map

*** Scope

| Component                | Count |
|--------------------------+-------|
| Message handlers         |     6 |
| Service classes          |    32 |
| Repository classes       |    41 |

Note: Services and repositories do NOT need interface changes. They continue
to accept ~database::context~ - only the handlers change how they create contexts.

